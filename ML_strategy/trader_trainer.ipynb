{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trader Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\brand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ray\\rllib\\utils\\framework.py:126: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 11:34:02,360\tINFO worker.py:1771 -- Started a local Ray instance.\n",
      "2024-07-05 11:34:06,001\tWARNING deprecation.py:50 -- DeprecationWarning: `algo = Algorithm(env='<class 'trading_env.TradingEnv'>', ...)` has been deprecated. Use `algo = AlgorithmConfig().environment('<class 'trading_env.TradingEnv'>').build()` instead. This will raise an error in the future!\n",
      "c:\\Users\\brand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py:516: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "c:\\Users\\brand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ray\\tune\\logger\\unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "c:\\Users\\brand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ray\\tune\\logger\\unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "c:\\Users\\brand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ray\\tune\\logger\\unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "\u001b[36m(pid=48152)\u001b[0m WARNING:tensorflow:From c:\\Users\\brand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ray\\rllib\\utils\\framework.py:126: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\u001b[36m(pid=48152)\u001b[0m \n",
      "\u001b[36m(pid=28248)\u001b[0m \n",
      "2024-07-05 11:34:16,939\tINFO trainable.py:161 -- Trainable.setup took 10.904 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2024-07-05 11:34:16,942\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training episode 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 11:34:21,689\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.multi_gpu_train_one_step` has been deprecated. This will raise an error in the future!\n",
      "2024-07-05 11:34:33,694\tWARNING ppo.py:696 -- The mean reward returned from the environment is 41.07015609741211 but the vf_clip_param is set to 10.0. Consider increasing it for policy: default_policy to improve value function convergence.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 4000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 4000\n",
      "  num_agent_steps_trained: 4000\n",
      "  num_env_steps_sampled: 4000\n",
      "  num_env_steps_trained: 4000\n",
      "custom_metrics: {}\n",
      "date: 2024-07-05_11-34-33\n",
      "done: false\n",
      "env_runners:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.008855760097503662\n",
      "    StateBufferConnector_ms: 0.0061083585023880005\n",
      "    ViewRequirementAgentConnector_ms: 0.09066946804523468\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 61.0\n",
      "  episode_media: {}\n",
      "  episode_return_max: 8608.272633955472\n",
      "  episode_return_mean: 2567.6612590262503\n",
      "  episode_return_min: 472.9123830118263\n",
      "  episode_reward_max: 8608.272633955472\n",
      "  episode_reward_mean: 2567.6612590262503\n",
      "  episode_reward_min: 472.9123830118263\n",
      "  episodes_this_iter: 64\n",
      "  episodes_timesteps_total: 3904\n",
      "  hist_stats:\n",
      "    episode_lengths: [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61]\n",
      "    episode_reward: [1101.4069839730662, 2176.554734215383, 8608.272633955472, 1092.4048174351399,\n",
      "      2799.7034632228224, 809.0066908142005, 2078.211846929043, 3007.651089465033,\n",
      "      1712.198107458707, 1532.2302371142787, 879.3541120442966, 7467.265687121047,\n",
      "      849.3903643874145, 1924.9851697465506, 889.3450785588908, 1359.756881857623,\n",
      "      647.8685363295638, 642.801004533966, 1589.0927194864978, 988.6126048547003,\n",
      "      710.4871408778474, 5550.25544701408, 5627.983919714854, 881.2763735323716, 2455.969066823386,\n",
      "      913.9967559412414, 554.5423562342307, 7692.5296498195075, 5970.062742764732,\n",
      "      1618.3681409311619, 4617.112687313098, 654.7204523568128, 8592.627952758785,\n",
      "      1427.6762898406946, 1261.5269399280728, 5327.258169212085, 1909.194053295158,\n",
      "      7417.703666109939, 2376.766783467802, 837.1393147203111, 611.2294988878949,\n",
      "      2765.189002171456, 7118.094301460194, 1273.4989661324894, 5219.614203302311,\n",
      "      1553.5674833412086, 572.4289997651207, 1344.9858986026811, 1967.9686362762864,\n",
      "      629.2861562486091, 672.0282594333781, 550.5273016544301, 657.6437661725222,\n",
      "      3116.6225251054652, 947.957136847521, 6667.319932530543, 472.9123830118263,\n",
      "      1183.0122699332073, 6245.071277284079, 1029.3231958894166, 1767.604969523251,\n",
      "      1142.3927697196748, 1908.71883105099, 6360.012147175601]\n",
      "  num_episodes: 64\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1209802713351271\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8696062692340049\n",
      "    mean_inference_ms: 0.8415232176544786\n",
      "    mean_raw_obs_processing_ms: 0.5007093397156708\n",
      "episode_media: {}\n",
      "hostname: Brandons-PC\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.4375493693095382\n",
      "        entropy_coeff: 0.0\n",
      "        grad_gnorm: 1.1073556554894293\n",
      "        kl: 0.0035728120421227166\n",
      "        policy_loss: -0.01783977293118995\n",
      "        total_loss: 9.977843521487328\n",
      "        vf_explained_var: -4.6030167610414567e-07\n",
      "        vf_loss: 9.994968761936311\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "      num_grad_updates_lifetime: 465.5\n",
      "  num_agent_steps_sampled: 4000\n",
      "  num_agent_steps_trained: 4000\n",
      "  num_env_steps_sampled: 4000\n",
      "  num_env_steps_trained: 4000\n",
      "iterations_since_restore: 1\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 4000\n",
      "num_agent_steps_sampled_lifetime: 4000\n",
      "num_agent_steps_trained: 4000\n",
      "num_env_steps_sampled: 4000\n",
      "num_env_steps_sampled_lifetime: 4000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_sampled_throughput_per_sec: 238.96700127123623\n",
      "num_env_steps_trained: 4000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_env_steps_trained_throughput_per_sec: 238.96700127123623\n",
      "num_healthy_workers: 2\n",
      "num_in_flight_async_sample_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 8.266666666666666\n",
      "  ram_util_percent: 87.77083333333333\n",
      "pid: 52792\n",
      "time_since_restore: 16.74844527244568\n",
      "time_this_iter_s: 16.74844527244568\n",
      "time_total_s: 16.74844527244568\n",
      "timers:\n",
      "  learn_throughput: 333.53\n",
      "  learn_time_ms: 11992.919\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  restore_workers_time_ms: 0.0\n",
      "  sample_time_ms: 4730.828\n",
      "  synch_weights_time_ms: 10.962\n",
      "  training_iteration_time_ms: 16738.713\n",
      "  training_step_time_ms: 16737.715\n",
      "timestamp: 1720197273\n",
      "timesteps_total: 4000\n",
      "training_iteration: 1\n",
      "trial_id: default\n",
      "\n",
      "Training episode 1\n",
      "agent_timesteps_total: 8000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 8000\n",
      "  num_agent_steps_trained: 8000\n",
      "  num_env_steps_sampled: 8000\n",
      "  num_env_steps_trained: 8000\n",
      "custom_metrics: {}\n",
      "date: 2024-07-05_11-34-55\n",
      "done: false\n",
      "env_runners:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0053670406341552734\n",
      "    StateBufferConnector_ms: 0.003904581069946289\n",
      "    ViewRequirementAgentConnector_ms: 0.09255194664001465\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 61.0\n",
      "  episode_media: {}\n",
      "  episode_return_max: 8592.627952758785\n",
      "  episode_return_mean: 2042.5241721649156\n",
      "  episode_return_min: 472.9123830118263\n",
      "  episode_reward_max: 8592.627952758785\n",
      "  episode_reward_mean: 2042.5241721649156\n",
      "  episode_reward_min: 472.9123830118263\n",
      "  episodes_this_iter: 66\n",
      "  episodes_timesteps_total: 6100\n",
      "  hist_stats:\n",
      "    episode_lengths: [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61]\n",
      "    episode_reward: [4617.112687313098, 654.7204523568128, 8592.627952758785, 1427.6762898406946,\n",
      "      1261.5269399280728, 5327.258169212085, 1909.194053295158, 7417.703666109939,\n",
      "      2376.766783467802, 837.1393147203111, 611.2294988878949, 2765.189002171456,\n",
      "      7118.094301460194, 1273.4989661324894, 5219.614203302311, 1553.5674833412086,\n",
      "      572.4289997651207, 1344.9858986026811, 1967.9686362762864, 629.2861562486091,\n",
      "      672.0282594333781, 550.5273016544301, 657.6437661725222, 3116.6225251054652,\n",
      "      947.957136847521, 6667.319932530543, 472.9123830118263, 1183.0122699332073,\n",
      "      6245.071277284079, 1029.3231958894166, 1767.604969523251, 1142.3927697196748,\n",
      "      1908.71883105099, 6360.012147175601, 638.969402740792, 4463.062779780355, 1098.84101763251,\n",
      "      1065.4712265397748, 950.4980862723444, 4319.302239367911, 2171.278877762476,\n",
      "      4481.773315050039, 2930.9845066246885, 833.439236545989, 1461.7650205409495,\n",
      "      3112.3888179299056, 2330.583035239198, 2204.4590703451695, 829.0493128221394,\n",
      "      2793.4010386185346, 1199.0172279182264, 801.9369567806912, 828.3262628685495,\n",
      "      2156.455119728128, 983.5516786863103, 3399.6364201817482, 1143.5284690513906,\n",
      "      2566.917655211107, 5921.971385593095, 850.7543304061334, 989.088557363642, 3025.1129538048763,\n",
      "      3344.7848884397345, 1294.6397037564723, 888.1727147534775, 775.047308085059,\n",
      "      716.6379223863163, 1294.0765642655736, 890.9942074392048, 1753.7178735626717,\n",
      "      1663.7432966483161, 1132.1333734778107, 3232.213102381561, 3858.4973303107117,\n",
      "      712.4584813444061, 2332.411150613657, 1588.9223597930063, 1162.12404796784,\n",
      "      6116.285191726938, 489.37693881703217, 930.0720796083017, 1125.7109816545985,\n",
      "      1987.4972640949625, 831.7927720453906, 3240.787045476668, 544.4595566669243,\n",
      "      754.699976031103, 500.35325284202577, 975.741400214013, 903.3081970430906, 564.1849255646226,\n",
      "      1667.1573534538204, 534.7561979543634, 671.4917727088894, 773.63195669448, 892.0062191749666,\n",
      "      1790.0916126849372, 1055.4319074179998, 586.3379611738056, 926.3681062872193]\n",
      "  num_episodes: 66\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11930995956670813\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8459833230446009\n",
      "    mean_inference_ms: 0.8279679398053875\n",
      "    mean_raw_obs_processing_ms: 0.495550646641358\n",
      "episode_media: {}\n",
      "hostname: Brandons-PC\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.10000000000000002\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.3791365237646205\n",
      "        entropy_coeff: 0.0\n",
      "        grad_gnorm: 1.0514379567036065\n",
      "        kl: 0.008187013710760778\n",
      "        policy_loss: -0.022928294502649337\n",
      "        total_loss: 9.977890424830939\n",
      "        vf_explained_var: -8.311002485213741e-06\n",
      "        vf_loss: 10.0\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "      num_grad_updates_lifetime: 1395.5\n",
      "  num_agent_steps_sampled: 8000\n",
      "  num_agent_steps_trained: 8000\n",
      "  num_env_steps_sampled: 8000\n",
      "  num_env_steps_trained: 8000\n",
      "iterations_since_restore: 2\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 8000\n",
      "num_agent_steps_sampled_lifetime: 8000\n",
      "num_agent_steps_trained: 8000\n",
      "num_env_steps_sampled: 8000\n",
      "num_env_steps_sampled_lifetime: 8000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_sampled_throughput_per_sec: 183.77478695839523\n",
      "num_env_steps_trained: 8000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_env_steps_trained_throughput_per_sec: 183.77478695839523\n",
      "num_healthy_workers: 2\n",
      "num_in_flight_async_sample_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 14.616129032258064\n",
      "  ram_util_percent: 87.32580645161289\n",
      "pid: 52792\n",
      "time_since_restore: 38.522576093673706\n",
      "time_this_iter_s: 21.774130821228027\n",
      "time_total_s: 38.522576093673706\n",
      "timers:\n",
      "  learn_throughput: 273.311\n",
      "  learn_time_ms: 14635.363\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  restore_workers_time_ms: 0.0\n",
      "  sample_time_ms: 4600.624\n",
      "  synch_weights_time_ms: 13.747\n",
      "  training_iteration_time_ms: 19252.242\n",
      "  training_step_time_ms: 19251.743\n",
      "timestamp: 1720197295\n",
      "timesteps_total: 8000\n",
      "training_iteration: 2\n",
      "trial_id: default\n",
      "\n",
      "Training episode 2\n",
      "agent_timesteps_total: 12000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 12000\n",
      "  num_agent_steps_trained: 12000\n",
      "  num_env_steps_sampled: 12000\n",
      "  num_env_steps_trained: 12000\n",
      "custom_metrics: {}\n",
      "date: 2024-07-05_11-35-27\n",
      "done: false\n",
      "env_runners:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.003083944320678711\n",
      "    StateBufferConnector_ms: 0.0076329708099365234\n",
      "    ViewRequirementAgentConnector_ms: 0.11435365676879883\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 61.0\n",
      "  episode_media: {}\n",
      "  episode_return_max: 6521.977780545051\n",
      "  episode_return_mean: 1638.772443393461\n",
      "  episode_return_min: 384.9137893727301\n",
      "  episode_reward_max: 6521.977780545051\n",
      "  episode_reward_mean: 1638.772443393461\n",
      "  episode_reward_min: 384.9137893727301\n",
      "  episodes_this_iter: 66\n",
      "  episodes_timesteps_total: 6100\n",
      "  hist_stats:\n",
      "    episode_lengths: [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61]\n",
      "    episode_reward: [716.6379223863163, 1294.0765642655736, 890.9942074392048, 1753.7178735626717,\n",
      "      1663.7432966483161, 1132.1333734778107, 3232.213102381561, 3858.4973303107117,\n",
      "      712.4584813444061, 2332.411150613657, 1588.9223597930063, 1162.12404796784,\n",
      "      6116.285191726938, 489.37693881703217, 930.0720796083017, 1125.7109816545985,\n",
      "      1987.4972640949625, 831.7927720453906, 3240.787045476668, 544.4595566669243,\n",
      "      754.699976031103, 500.35325284202577, 975.741400214013, 903.3081970430906, 564.1849255646226,\n",
      "      1667.1573534538204, 534.7561979543634, 671.4917727088894, 773.63195669448, 892.0062191749666,\n",
      "      1790.0916126849372, 1055.4319074179998, 586.3379611738056, 926.3681062872193,\n",
      "      1842.6988697652973, 2226.815287499606, 780.0670526187539, 2122.428942496943,\n",
      "      1369.5794282841916, 986.9092934372536, 775.9826525717583, 1451.352622498197,\n",
      "      1403.155866042996, 809.7369665574271, 967.9392316218332, 1984.66288086772, 1736.4966458106871,\n",
      "      910.4488458494698, 885.955888465836, 833.0832996333908, 2042.9738039919418,\n",
      "      1046.3021800276333, 6397.326323447942, 1409.9956201189023, 384.9137893727301,\n",
      "      1741.6882847679317, 829.7582884193121, 1171.4224113310208, 2481.991053164527,\n",
      "      4715.749211355965, 1998.967498841815, 6521.977780545051, 727.4477421402096,\n",
      "      1073.8436183889473, 581.1137560484226, 772.3487259048968, 876.827949512291,\n",
      "      987.8439943698669, 1970.6053630634515, 2207.01614803668, 1608.2157490284449,\n",
      "      1184.899831098855, 2241.7472044977503, 769.665786984882, 2047.2668367538568,\n",
      "      2694.609182767346, 3920.2483645974494, 2047.606113191683, 1541.4005022081021,\n",
      "      944.2312405095861, 1485.3328663378466, 701.179280924625, 4180.3083520434375,\n",
      "      2627.7021360407466, 558.8154480242474, 2467.584981539819, 3259.753138770479,\n",
      "      494.6210424785114, 835.2487030672656, 708.220750948931, 517.4330810743648, 4680.910066835277,\n",
      "      1152.345404757138, 1936.6052447989043, 813.8877202896679, 3091.1351371026417,\n",
      "      2310.0779695159667, 2075.2012341140608, 1023.3577518702654, 730.7335207758163]\n",
      "  num_episodes: 66\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11736670199237306\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.8444005471639008\n",
      "    mean_inference_ms: 0.8305747769818004\n",
      "    mean_raw_obs_processing_ms: 0.4998332266269103\n",
      "episode_media: {}\n",
      "hostname: Brandons-PC\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.10000000000000002\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.3398956111682359\n",
      "        entropy_coeff: 0.0\n",
      "        grad_gnorm: 1.0808635689238066\n",
      "        kl: 0.008327846325660814\n",
      "        policy_loss: -0.022853707461067106\n",
      "        total_loss: 9.977979049887708\n",
      "        vf_explained_var: -1.781602059641192e-05\n",
      "        vf_loss: 10.0\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "      num_grad_updates_lifetime: 2325.5\n",
      "  num_agent_steps_sampled: 12000\n",
      "  num_agent_steps_trained: 12000\n",
      "  num_env_steps_sampled: 12000\n",
      "  num_env_steps_trained: 12000\n",
      "iterations_since_restore: 3\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 12000\n",
      "num_agent_steps_sampled_lifetime: 12000\n",
      "num_agent_steps_trained: 12000\n",
      "num_env_steps_sampled: 12000\n",
      "num_env_steps_sampled_lifetime: 12000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_sampled_throughput_per_sec: 123.87185641002392\n",
      "num_env_steps_trained: 12000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_env_steps_trained_throughput_per_sec: 123.87185641002392\n",
      "num_healthy_workers: 2\n",
      "num_in_flight_async_sample_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 25.35777777777778\n",
      "  ram_util_percent: 87.74666666666666\n",
      "pid: 52792\n",
      "time_since_restore: 70.82502794265747\n",
      "time_this_iter_s: 32.302451848983765\n",
      "time_total_s: 70.82502794265747\n",
      "timers:\n",
      "  learn_throughput: 211.514\n",
      "  learn_time_ms: 18911.29\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  restore_workers_time_ms: 0.0\n",
      "  sample_time_ms: 4671.745\n",
      "  synch_weights_time_ms: 13.265\n",
      "  training_iteration_time_ms: 23598.64\n",
      "  training_step_time_ms: 23598.307\n",
      "timestamp: 1720197327\n",
      "timesteps_total: 12000\n",
      "training_iteration: 3\n",
      "trial_id: default\n",
      "\n",
      "Training episode 3\n",
      "agent_timesteps_total: 16000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 16000\n",
      "  num_agent_steps_trained: 16000\n",
      "  num_env_steps_sampled: 16000\n",
      "  num_env_steps_trained: 16000\n",
      "custom_metrics: {}\n",
      "date: 2024-07-05_11-35-50\n",
      "done: false\n",
      "env_runners:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.010225534439086914\n",
      "    StateBufferConnector_ms: 0.01000213623046875\n",
      "    ViewRequirementAgentConnector_ms: 0.1209101676940918\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 61.0\n",
      "  episode_media: {}\n",
      "  episode_return_max: 8544.038258368824\n",
      "  episode_return_mean: 2402.786542344347\n",
      "  episode_return_min: 464.06939557367474\n",
      "  episode_reward_max: 8544.038258368824\n",
      "  episode_reward_mean: 2402.786542344347\n",
      "  episode_reward_min: 464.06939557367474\n",
      "  episodes_this_iter: 66\n",
      "  episodes_timesteps_total: 6100\n",
      "  hist_stats:\n",
      "    episode_lengths: [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61]\n",
      "    episode_reward: [876.827949512291, 987.8439943698669, 1970.6053630634515, 2207.01614803668,\n",
      "      1608.2157490284449, 1184.899831098855, 2241.7472044977503, 769.665786984882,\n",
      "      2047.2668367538568, 2694.609182767346, 3920.2483645974494, 2047.606113191683,\n",
      "      1541.4005022081021, 944.2312405095861, 1485.3328663378466, 701.179280924625,\n",
      "      4180.3083520434375, 2627.7021360407466, 558.8154480242474, 2467.584981539819,\n",
      "      3259.753138770479, 494.6210424785114, 835.2487030672656, 708.220750948931, 517.4330810743648,\n",
      "      4680.910066835277, 1152.345404757138, 1936.6052447989043, 813.8877202896679,\n",
      "      3091.1351371026417, 2310.0779695159667, 2075.2012341140608, 1023.3577518702654,\n",
      "      730.7335207758163, 2815.029572654677, 1921.0597909846156, 860.5146366786807,\n",
      "      619.8989671224535, 3590.0832685646988, 5288.997888532247, 1323.0992787679024,\n",
      "      464.06939557367474, 5528.128674103678, 5316.325838118846, 6755.097393826636,\n",
      "      1235.0029896717128, 993.0775483329759, 1044.394555422239, 851.4548008219668,\n",
      "      6178.156852567345, 2641.173891995443, 1021.8793450454395, 968.5024940603223,\n",
      "      6792.04099783424, 1042.5791970786456, 542.2405497712969, 2503.8342723629635,\n",
      "      657.2313646308756, 4888.495935271086, 8544.038258368824, 3793.43021407208, 632.0613552166885,\n",
      "      4402.406251946337, 5777.765629102599, 812.3544379463472, 1145.583467769317,\n",
      "      593.9945295252146, 1715.3831891137738, 1639.7373620521896, 1042.6745085739983,\n",
      "      1519.3026985295721, 2549.2982275484496, 7883.567003817514, 1185.0877288455863,\n",
      "      5744.876272915135, 8316.016015788779, 790.4005506958159, 4298.829257768155,\n",
      "      5555.933038502618, 4772.5837102020505, 3252.3211193490883, 2359.8394413547435,\n",
      "      5925.280684508891, 680.4482064102747, 776.9530510245656, 669.050030215828, 2948.084878222419,\n",
      "      1074.4395773358995, 751.141329892221, 2616.0515332304694, 2947.7769990398224,\n",
      "      1758.8915237423316, 6143.525351063015, 815.8832742364756, 476.3799695542399,\n",
      "      606.1978179278171, 2735.681051086474, 3079.772293226404, 764.8370034237687,\n",
      "      645.7677935639636]\n",
      "  num_episodes: 66\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12222481608078953\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.884766922311982\n",
      "    mean_inference_ms: 0.86697905731829\n",
      "    mean_raw_obs_processing_ms: 0.520682882459337\n",
      "episode_media: {}\n",
      "hostname: Brandons-PC\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.10000000000000002\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.317430639779696\n",
      "        entropy_coeff: 0.0\n",
      "        grad_gnorm: 1.0770677418478074\n",
      "        kl: 0.006555967069093591\n",
      "        policy_loss: -0.02168940607538467\n",
      "        total_loss: 9.966581157971454\n",
      "        vf_explained_var: -1.9169622851956277e-06\n",
      "        vf_loss: 9.987614961849745\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "      num_grad_updates_lifetime: 3255.5\n",
      "  num_agent_steps_sampled: 16000\n",
      "  num_agent_steps_trained: 16000\n",
      "  num_env_steps_sampled: 16000\n",
      "  num_env_steps_trained: 16000\n",
      "iterations_since_restore: 4\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 16000\n",
      "num_agent_steps_sampled_lifetime: 16000\n",
      "num_agent_steps_trained: 16000\n",
      "num_env_steps_sampled: 16000\n",
      "num_env_steps_sampled_lifetime: 16000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_sampled_throughput_per_sec: 174.93580573589782\n",
      "num_env_steps_trained: 16000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_env_steps_trained_throughput_per_sec: 174.93580573589782\n",
      "num_healthy_workers: 2\n",
      "num_in_flight_async_sample_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 21.821875\n",
      "  ram_util_percent: 89.009375\n",
      "pid: 52792\n",
      "time_since_restore: 93.69885444641113\n",
      "time_this_iter_s: 22.873826503753662\n",
      "time_total_s: 93.69885444641113\n",
      "timers:\n",
      "  learn_throughput: 216.75\n",
      "  learn_time_ms: 18454.413\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  restore_workers_time_ms: 0.0\n",
      "  sample_time_ms: 4945.746\n",
      "  synch_weights_time_ms: 13.198\n",
      "  training_iteration_time_ms: 23415.362\n",
      "  training_step_time_ms: 23415.113\n",
      "timestamp: 1720197350\n",
      "timesteps_total: 16000\n",
      "training_iteration: 4\n",
      "trial_id: default\n",
      "\n",
      "Training episode 4\n",
      "agent_timesteps_total: 20000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 20000\n",
      "  num_agent_steps_trained: 20000\n",
      "  num_env_steps_sampled: 20000\n",
      "  num_env_steps_trained: 20000\n",
      "custom_metrics: {}\n",
      "date: 2024-07-05_11-36-16\n",
      "done: false\n",
      "env_runners:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.009130477905273438\n",
      "    StateBufferConnector_ms: 0.005228757858276367\n",
      "    ViewRequirementAgentConnector_ms: 0.10560369491577148\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 61.0\n",
      "  episode_media: {}\n",
      "  episode_return_max: 8316.016015788779\n",
      "  episode_return_mean: 2053.799705088789\n",
      "  episode_return_min: 338.84092297528923\n",
      "  episode_reward_max: 8316.016015788779\n",
      "  episode_reward_mean: 2053.799705088789\n",
      "  episode_reward_min: 338.84092297528923\n",
      "  episodes_this_iter: 64\n",
      "  episodes_timesteps_total: 6100\n",
      "  hist_stats:\n",
      "    episode_lengths: [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61]\n",
      "    episode_reward: [812.3544379463472, 1145.583467769317, 593.9945295252146, 1715.3831891137738,\n",
      "      1639.7373620521896, 1042.6745085739983, 1519.3026985295721, 2549.2982275484496,\n",
      "      7883.567003817514, 1185.0877288455863, 5744.876272915135, 8316.016015788779,\n",
      "      790.4005506958159, 4298.829257768155, 5555.933038502618, 4772.5837102020505,\n",
      "      3252.3211193490883, 2359.8394413547435, 5925.280684508891, 680.4482064102747,\n",
      "      776.9530510245656, 669.050030215828, 2948.084878222419, 1074.4395773358995,\n",
      "      751.141329892221, 2616.0515332304694, 2947.7769990398224, 1758.8915237423316,\n",
      "      6143.525351063015, 815.8832742364756, 476.3799695542399, 606.1978179278171,\n",
      "      2735.681051086474, 3079.772293226404, 764.8370034237687, 645.7677935639636,\n",
      "      763.5972609508706, 936.8234309248044, 867.4130365612183, 338.84092297528923,\n",
      "      1353.8944482134793, 3752.908595868422, 817.2207382102796, 714.4760000355192,\n",
      "      844.34589663371, 1484.3725768758818, 525.8892759434963, 811.8486965847976, 1348.1319973838692,\n",
      "      1517.6585274678982, 7189.877669824182, 5975.709398941907, 1061.379024150357,\n",
      "      2153.262272931785, 583.8360050145203, 1771.7665898244973, 1438.0376928467397,\n",
      "      1587.9271104529955, 3203.3744643952386, 704.0462934918012, 6649.280022894921,\n",
      "      750.4365908107711, 3829.40015001491, 912.2193104230587, 4763.598217888764, 832.9200134151481,\n",
      "      621.2330917526613, 1999.5922521746834, 945.3712884117815, 641.9404727548514,\n",
      "      3542.0347842762317, 1952.8672141531417, 1208.4547483344088, 486.5619539619888,\n",
      "      819.974933203151, 2348.5195690550795, 690.8759639238892, 1482.7417905131517,\n",
      "      6451.256902367922, 1252.879597701925, 1739.021710327193, 923.7809062109526,\n",
      "      775.6315916558844, 5211.205763715005, 967.2841957593098, 801.1154140991649,\n",
      "      775.7662575284206, 1111.6816181656566, 1497.3503579986377, 675.5644996689218,\n",
      "      5478.902343592519, 732.4339714369769, 625.404135397997, 948.6903013547021, 1987.6695861167527,\n",
      "      627.7374638745289, 1387.6642981287835, 735.0007753549392, 1050.9781951145528,\n",
      "      2804.3454008388303]\n",
      "  num_episodes: 64\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12544865680219477\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9088954170451672\n",
      "    mean_inference_ms: 0.8862091051739024\n",
      "    mean_raw_obs_processing_ms: 0.5318141074107188\n",
      "episode_media: {}\n",
      "hostname: Brandons-PC\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.10000000000000002\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.2111851902418238\n",
      "        entropy_coeff: 0.0\n",
      "        grad_gnorm: 1.143758415022204\n",
      "        kl: 0.013651598322210325\n",
      "        policy_loss: -0.025129642205372935\n",
      "        total_loss: 9.976235526095154\n",
      "        vf_explained_var: -1.785377020476967e-05\n",
      "        vf_loss: 10.0\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "      num_grad_updates_lifetime: 4185.5\n",
      "  num_agent_steps_sampled: 20000\n",
      "  num_agent_steps_trained: 20000\n",
      "  num_env_steps_sampled: 20000\n",
      "  num_env_steps_trained: 20000\n",
      "iterations_since_restore: 5\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 20000\n",
      "num_agent_steps_sampled_lifetime: 20000\n",
      "num_agent_steps_trained: 20000\n",
      "num_env_steps_sampled: 20000\n",
      "num_env_steps_sampled_lifetime: 20000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_sampled_throughput_per_sec: 158.20009851146156\n",
      "num_env_steps_trained: 20000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_env_steps_trained_throughput_per_sec: 158.20009851146156\n",
      "num_healthy_workers: 2\n",
      "num_in_flight_async_sample_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 12.111428571428572\n",
      "  ram_util_percent: 87.96285714285715\n",
      "pid: 52792\n",
      "time_since_restore: 118.99228549003601\n",
      "time_this_iter_s: 25.293431043624878\n",
      "time_total_s: 118.99228549003601\n",
      "timers:\n",
      "  learn_throughput: 212.782\n",
      "  learn_time_ms: 18798.607\n",
      "  load_throughput: 39512991.05\n",
      "  load_time_ms: 0.101\n",
      "  restore_workers_time_ms: 0.2\n",
      "  sample_time_ms: 4976.417\n",
      "  synch_weights_time_ms: 12.046\n",
      "  training_iteration_time_ms: 23789.177\n",
      "  training_step_time_ms: 23788.777\n",
      "timestamp: 1720197376\n",
      "timesteps_total: 20000\n",
      "training_iteration: 5\n",
      "trial_id: default\n",
      "\n",
      "Training episode 5\n",
      "agent_timesteps_total: 24000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 24000\n",
      "  num_agent_steps_trained: 24000\n",
      "  num_env_steps_sampled: 24000\n",
      "  num_env_steps_trained: 24000\n",
      "custom_metrics: {}\n",
      "date: 2024-07-05_11-36-52\n",
      "done: false\n",
      "env_runners:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.007982969284057617\n",
      "    StateBufferConnector_ms: 0.007124900817871094\n",
      "    ViewRequirementAgentConnector_ms: 0.11760473251342773\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 61.0\n",
      "  episode_media: {}\n",
      "  episode_return_max: 6635.3762449290025\n",
      "  episode_return_mean: 1826.2326533046235\n",
      "  episode_return_min: 423.473133401495\n",
      "  episode_reward_max: 6635.3762449290025\n",
      "  episode_reward_mean: 1826.2326533046235\n",
      "  episode_reward_min: 423.473133401495\n",
      "  episodes_this_iter: 66\n",
      "  episodes_timesteps_total: 6100\n",
      "  hist_stats:\n",
      "    episode_lengths: [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61]\n",
      "    episode_reward: [621.2330917526613, 1999.5922521746834, 945.3712884117815, 641.9404727548514,\n",
      "      3542.0347842762317, 1952.8672141531417, 1208.4547483344088, 486.5619539619888,\n",
      "      819.974933203151, 2348.5195690550795, 690.8759639238892, 1482.7417905131517,\n",
      "      6451.256902367922, 1252.879597701925, 1739.021710327193, 923.7809062109526,\n",
      "      775.6315916558844, 5211.205763715005, 967.2841957593098, 801.1154140991649,\n",
      "      775.7662575284206, 1111.6816181656566, 1497.3503579986377, 675.5644996689218,\n",
      "      5478.902343592519, 732.4339714369769, 625.404135397997, 948.6903013547021, 1987.6695861167527,\n",
      "      627.7374638745289, 1387.6642981287835, 735.0007753549392, 1050.9781951145528,\n",
      "      2804.3454008388303, 3213.234986525951, 958.8702880558506, 1343.0588298624566,\n",
      "      4633.142066253316, 6370.171502050307, 1050.7365133532364, 1622.2431781355344,\n",
      "      1655.1916303226724, 1902.949296669281, 620.6435852649986, 623.9766951416093,\n",
      "      1630.2754757833372, 712.0936358593992, 3261.399880070509, 1225.9444127002055,\n",
      "      5749.125458580155, 941.7694258496682, 1195.0092861121238, 795.4336416167714,\n",
      "      804.7796660666991, 1327.632192420363, 553.3180257804848, 2069.9879144166584,\n",
      "      2984.776761955398, 850.8266321477884, 823.5615602030243, 676.42857405925, 4943.223061111894,\n",
      "      844.7829783924171, 3293.6762802652884, 2737.7214447722167, 492.2890804028445,\n",
      "      673.7679370843373, 718.8787211714124, 3769.311803912139, 6085.138189548647,\n",
      "      1196.6462903859558, 595.6046790247391, 2204.839054095931, 634.4524704424476,\n",
      "      615.2385555367551, 622.5766245115612, 4136.691405904983, 6635.3762449290025,\n",
      "      893.7625224959684, 957.2024007954961, 817.1722446101597, 4119.07752556406, 1235.3678713993252,\n",
      "      2022.6239509681902, 584.586793485666, 4274.041030061509, 641.8997838661851,\n",
      "      630.054572724765, 6634.641050422309, 537.4699544551272, 740.2177558639079, 854.4543108680283,\n",
      "      745.1831900941417, 777.8749640254612, 2152.0511954014596, 1465.0818889535003,\n",
      "      761.208260643769, 423.473133401495, 5481.880349774145, 775.6112949134149]\n",
      "  num_episodes: 66\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12967566447993076\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9269257825298893\n",
      "    mean_inference_ms: 0.9041703483599047\n",
      "    mean_raw_obs_processing_ms: 0.5429301770195277\n",
      "episode_media: {}\n",
      "hostname: Brandons-PC\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.10000000000000002\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.1479542383583643\n",
      "        entropy_coeff: 0.0\n",
      "        grad_gnorm: 1.2508996283495297\n",
      "        kl: 0.013799683077877978\n",
      "        policy_loss: -0.031948289803920255\n",
      "        total_loss: 9.96943167512135\n",
      "        vf_explained_var: 7.4217396397744455e-06\n",
      "        vf_loss: 10.0\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "      num_grad_updates_lifetime: 5115.5\n",
      "  num_agent_steps_sampled: 24000\n",
      "  num_agent_steps_trained: 24000\n",
      "  num_env_steps_sampled: 24000\n",
      "  num_env_steps_trained: 24000\n",
      "iterations_since_restore: 6\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 24000\n",
      "num_agent_steps_sampled_lifetime: 24000\n",
      "num_agent_steps_trained: 24000\n",
      "num_env_steps_sampled: 24000\n",
      "num_env_steps_sampled_lifetime: 24000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_sampled_throughput_per_sec: 109.46702660978204\n",
      "num_env_steps_trained: 24000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_env_steps_trained_throughput_per_sec: 109.46702660978204\n",
      "num_healthy_workers: 2\n",
      "num_in_flight_async_sample_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 31.186274509803923\n",
      "  ram_util_percent: 88.50392156862745\n",
      "pid: 52792\n",
      "time_since_restore: 155.54095673561096\n",
      "time_this_iter_s: 36.54867124557495\n",
      "time_total_s: 155.54095673561096\n",
      "timers:\n",
      "  learn_throughput: 192.447\n",
      "  learn_time_ms: 20784.939\n",
      "  load_throughput: 15726182.784\n",
      "  load_time_ms: 0.254\n",
      "  restore_workers_time_ms: 0.167\n",
      "  sample_time_ms: 5115.6\n",
      "  synch_weights_time_ms: 11.964\n",
      "  training_iteration_time_ms: 25914.428\n",
      "  training_step_time_ms: 25914.095\n",
      "timestamp: 1720197412\n",
      "timesteps_total: 24000\n",
      "training_iteration: 6\n",
      "trial_id: default\n",
      "\n",
      "Training episode 6\n",
      "agent_timesteps_total: 28000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 28000\n",
      "  num_agent_steps_trained: 28000\n",
      "  num_env_steps_sampled: 28000\n",
      "  num_env_steps_trained: 28000\n",
      "custom_metrics: {}\n",
      "date: 2024-07-05_11-37-19\n",
      "done: false\n",
      "env_runners:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.006963968276977539\n",
      "    StateBufferConnector_ms: 0.006044864654541016\n",
      "    ViewRequirementAgentConnector_ms: 0.13672828674316406\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 61.0\n",
      "  episode_media: {}\n",
      "  episode_return_max: 6635.3762449290025\n",
      "  episode_return_mean: 1711.9580181800254\n",
      "  episode_return_min: 384.52909774173355\n",
      "  episode_reward_max: 6635.3762449290025\n",
      "  episode_reward_mean: 1711.9580181800254\n",
      "  episode_reward_min: 384.52909774173355\n",
      "  episodes_this_iter: 66\n",
      "  episodes_timesteps_total: 6100\n",
      "  hist_stats:\n",
      "    episode_lengths: [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61]\n",
      "    episode_reward: [673.7679370843373, 718.8787211714124, 3769.311803912139, 6085.138189548647,\n",
      "      1196.6462903859558, 595.6046790247391, 2204.839054095931, 634.4524704424476,\n",
      "      615.2385555367551, 622.5766245115612, 4136.691405904983, 6635.3762449290025,\n",
      "      893.7625224959684, 957.2024007954961, 817.1722446101597, 4119.07752556406, 1235.3678713993252,\n",
      "      2022.6239509681902, 584.586793485666, 4274.041030061509, 641.8997838661851,\n",
      "      630.054572724765, 6634.641050422309, 537.4699544551272, 740.2177558639079, 854.4543108680283,\n",
      "      745.1831900941417, 777.8749640254612, 2152.0511954014596, 1465.0818889535003,\n",
      "      761.208260643769, 423.473133401495, 5481.880349774145, 775.6112949134149, 1008.2413233095767,\n",
      "      4164.33043663827, 553.4061975458421, 1286.1427406240764, 630.5641633999993,\n",
      "      604.0498834422726, 1580.8570546541664, 2186.6447565152894, 1484.1179839928845,\n",
      "      3839.0841535295363, 4212.888025429283, 792.0844603561981, 1999.2190804660117,\n",
      "      1325.1712405886715, 2727.7159807556072, 1360.1843890296757, 2040.3473589092505,\n",
      "      717.1312004420981, 1146.190807260053, 492.1636979825691, 1099.910053641066,\n",
      "      2007.2388851072774, 2518.324390117861, 3339.2750605376787, 506.30519627381443,\n",
      "      1837.3611867392246, 1877.8187738314273, 384.52909774173355, 395.645915371679,\n",
      "      5140.416009338068, 1033.751156222025, 1443.117597242661, 1010.8064958376318,\n",
      "      4787.815006801493, 637.7434878795021, 543.6415046412054, 3403.7469012892916,\n",
      "      530.4329416809679, 1328.1162038786551, 448.0565240122321, 618.4086082338224,\n",
      "      2187.821800796427, 1386.3109408508942, 905.5855218272768, 1406.2427847758056,\n",
      "      967.1822138651745, 738.8209765248228, 1228.4425638684393, 984.6672010888558,\n",
      "      716.6402636472021, 3388.079578658366, 1404.6720317361078, 1187.4498406257403,\n",
      "      1052.1159682686334, 547.1115393130336, 2744.9590949543167, 745.1827292416723,\n",
      "      567.4695376462091, 933.0142893684385, 4790.3793513436285, 747.5356440982889,\n",
      "      875.9826676922537, 3595.3469863202035, 479.1390129684662, 1010.3211638190236,\n",
      "      2148.8741620766205]\n",
      "  num_episodes: 66\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.13400754871001466\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9635199175588756\n",
      "    mean_inference_ms: 0.9450482501491192\n",
      "    mean_raw_obs_processing_ms: 0.5640384438100805\n",
      "episode_media: {}\n",
      "hostname: Brandons-PC\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.10000000000000002\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.0744528038527377\n",
      "        entropy_coeff: 0.0\n",
      "        grad_gnorm: 1.3852470516197144\n",
      "        kl: 0.012942759238696108\n",
      "        policy_loss: -0.030187340295042404\n",
      "        total_loss: 9.964086677182106\n",
      "        vf_explained_var: -4.901873168124947e-06\n",
      "        vf_loss: 9.992979734687395\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "      num_grad_updates_lifetime: 6045.5\n",
      "  num_agent_steps_sampled: 28000\n",
      "  num_agent_steps_trained: 28000\n",
      "  num_env_steps_sampled: 28000\n",
      "  num_env_steps_trained: 28000\n",
      "iterations_since_restore: 7\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 28000\n",
      "num_agent_steps_sampled_lifetime: 28000\n",
      "num_agent_steps_trained: 28000\n",
      "num_env_steps_sampled: 28000\n",
      "num_env_steps_sampled_lifetime: 28000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_sampled_throughput_per_sec: 148.26459048127603\n",
      "num_env_steps_trained: 28000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_env_steps_trained_throughput_per_sec: 148.26459048127603\n",
      "num_healthy_workers: 2\n",
      "num_in_flight_async_sample_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 48.85526315789473\n",
      "  ram_util_percent: 89.05263157894737\n",
      "pid: 52792\n",
      "time_since_restore: 182.52826929092407\n",
      "time_this_iter_s: 26.98731255531311\n",
      "time_total_s: 182.52826929092407\n",
      "timers:\n",
      "  learn_throughput: 193.257\n",
      "  learn_time_ms: 20697.853\n",
      "  load_throughput: 11081384.412\n",
      "  load_time_ms: 0.361\n",
      "  restore_workers_time_ms: 0.143\n",
      "  sample_time_ms: 5355.296\n",
      "  synch_weights_time_ms: 11.396\n",
      "  training_iteration_time_ms: 26066.48\n",
      "  training_step_time_ms: 26066.195\n",
      "timestamp: 1720197439\n",
      "timesteps_total: 28000\n",
      "training_iteration: 7\n",
      "trial_id: default\n",
      "\n",
      "Training episode 7\n",
      "agent_timesteps_total: 32000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 32000\n",
      "  num_agent_steps_trained: 32000\n",
      "  num_env_steps_sampled: 32000\n",
      "  num_env_steps_trained: 32000\n",
      "custom_metrics: {}\n",
      "date: 2024-07-05_11-37-46\n",
      "done: false\n",
      "env_runners:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.003968000411987305\n",
      "    StateBufferConnector_ms: 0.006003618240356445\n",
      "    ViewRequirementAgentConnector_ms: 0.12314701080322266\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 61.0\n",
      "  episode_media: {}\n",
      "  episode_return_max: 6664.090425451682\n",
      "  episode_return_mean: 1436.0065276496086\n",
      "  episode_return_min: 448.0565240122321\n",
      "  episode_reward_max: 6664.090425451682\n",
      "  episode_reward_mean: 1436.0065276496086\n",
      "  episode_reward_min: 448.0565240122321\n",
      "  episodes_this_iter: 66\n",
      "  episodes_timesteps_total: 6100\n",
      "  hist_stats:\n",
      "    episode_lengths: [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61]\n",
      "    episode_reward: [1010.8064958376318, 4787.815006801493, 637.7434878795021, 543.6415046412054,\n",
      "      3403.7469012892916, 530.4329416809679, 1328.1162038786551, 448.0565240122321,\n",
      "      618.4086082338224, 2187.821800796427, 1386.3109408508942, 905.5855218272768,\n",
      "      1406.2427847758056, 967.1822138651745, 738.8209765248228, 1228.4425638684393,\n",
      "      984.6672010888558, 716.6402636472021, 3388.079578658366, 1404.6720317361078,\n",
      "      1187.4498406257403, 1052.1159682686334, 547.1115393130336, 2744.9590949543167,\n",
      "      745.1827292416723, 567.4695376462091, 933.0142893684385, 4790.3793513436285,\n",
      "      747.5356440982889, 875.9826676922537, 3595.3469863202035, 479.1390129684662,\n",
      "      1010.3211638190236, 2148.8741620766205, 583.8309076487931, 1146.9102910613542,\n",
      "      2244.6565122270076, 2345.430643229633, 795.8202993693081, 471.8372545420046,\n",
      "      1063.2997540713104, 675.8708176684511, 4307.527633397311, 499.85792584720946,\n",
      "      818.5957097841987, 719.8443525641032, 1888.0968582223513, 490.57627613309637,\n",
      "      701.0764274155442, 590.4145216466395, 572.3479194510635, 458.1063588045537,\n",
      "      772.4190129282103, 623.2239698029477, 850.1236248348832, 1456.7242324060207,\n",
      "      1894.5145075516004, 953.690084264274, 558.2567680126658, 514.4133660988962,\n",
      "      3045.09310598697, 1303.235035130615, 6664.090425451682, 542.7704176336932, 651.9646481451065,\n",
      "      1938.9228145208485, 777.8024234706365, 883.1731506400656, 2561.1555303664773,\n",
      "      770.7097906166343, 1174.121614193803, 614.7154609395818, 551.1342520359435,\n",
      "      1882.065828171001, 674.7570780535601, 675.5658345003835, 610.5538838365953,\n",
      "      689.6695872415441, 2431.6660055111342, 704.46343144, 1280.4729538275194, 1593.8783078443153,\n",
      "      691.1117489147712, 2278.497887964921, 3020.6508961047416, 2496.23635848798,\n",
      "      895.0327043563617, 1737.0278599244639, 767.953674524085, 633.6234196359142,\n",
      "      652.4730178949907, 849.3171669157824, 4159.39080331878, 1037.736922412328, 2120.204144741036,\n",
      "      678.6962218624631, 1070.3890477315838, 4946.780673299452, 4664.239975562492,\n",
      "      833.7270931364699]\n",
      "  num_episodes: 66\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.13542731380212195\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9835285544874265\n",
      "    mean_inference_ms: 0.9664413936780744\n",
      "    mean_raw_obs_processing_ms: 0.5752703595612257\n",
      "episode_media: {}\n",
      "hostname: Brandons-PC\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.10000000000000002\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.0160207145957536\n",
      "        entropy_coeff: 0.0\n",
      "        grad_gnorm: 1.4712710937184672\n",
      "        kl: 0.012322614311008416\n",
      "        policy_loss: -0.024804650479426947\n",
      "        total_loss: 9.976427611484322\n",
      "        vf_explained_var: -2.000677970147902e-05\n",
      "        vf_loss: 10.0\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "      num_grad_updates_lifetime: 6975.5\n",
      "  num_agent_steps_sampled: 32000\n",
      "  num_agent_steps_trained: 32000\n",
      "  num_env_steps_sampled: 32000\n",
      "  num_env_steps_trained: 32000\n",
      "iterations_since_restore: 8\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 32000\n",
      "num_agent_steps_sampled_lifetime: 32000\n",
      "num_agent_steps_trained: 32000\n",
      "num_env_steps_sampled: 32000\n",
      "num_env_steps_sampled_lifetime: 32000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_sampled_throughput_per_sec: 152.08835555216154\n",
      "num_env_steps_trained: 32000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_env_steps_trained_throughput_per_sec: 152.08835555216154\n",
      "num_healthy_workers: 2\n",
      "num_in_flight_async_sample_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 14.18918918918919\n",
      "  ram_util_percent: 88.63513513513513\n",
      "pid: 52792\n",
      "time_since_restore: 208.83808279037476\n",
      "time_this_iter_s: 26.309813499450684\n",
      "time_total_s: 208.83808279037476\n",
      "timers:\n",
      "  learn_throughput: 193.389\n",
      "  learn_time_ms: 20683.706\n",
      "  load_throughput: 12664439.328\n",
      "  load_time_ms: 0.316\n",
      "  restore_workers_time_ms: 0.125\n",
      "  sample_time_ms: 5399.327\n",
      "  synch_weights_time_ms: 11.005\n",
      "  training_iteration_time_ms: 26095.733\n",
      "  training_step_time_ms: 26095.483\n",
      "timestamp: 1720197466\n",
      "timesteps_total: 32000\n",
      "training_iteration: 8\n",
      "trial_id: default\n",
      "\n",
      "Training episode 8\n",
      "agent_timesteps_total: 36000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 36000\n",
      "  num_agent_steps_trained: 36000\n",
      "  num_env_steps_sampled: 36000\n",
      "  num_env_steps_trained: 36000\n",
      "custom_metrics: {}\n",
      "date: 2024-07-05_11-38-08\n",
      "done: false\n",
      "env_runners:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.003988742828369141\n",
      "    StateBufferConnector_ms: 0.008001089096069336\n",
      "    ViewRequirementAgentConnector_ms: 0.1150517463684082\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 61.0\n",
      "  episode_media: {}\n",
      "  episode_return_max: 5927.717844719117\n",
      "  episode_return_mean: 1680.002078682288\n",
      "  episode_return_min: 327.7809400161942\n",
      "  episode_reward_max: 5927.717844719117\n",
      "  episode_reward_mean: 1680.002078682288\n",
      "  episode_reward_min: 327.7809400161942\n",
      "  episodes_this_iter: 66\n",
      "  episodes_timesteps_total: 6100\n",
      "  hist_stats:\n",
      "    episode_lengths: [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61]\n",
      "    episode_reward: [777.8024234706365, 883.1731506400656, 2561.1555303664773, 770.7097906166343,\n",
      "      1174.121614193803, 614.7154609395818, 551.1342520359435, 1882.065828171001,\n",
      "      674.7570780535601, 675.5658345003835, 610.5538838365953, 689.6695872415441,\n",
      "      2431.6660055111342, 704.46343144, 1280.4729538275194, 1593.8783078443153, 691.1117489147712,\n",
      "      2278.497887964921, 3020.6508961047416, 2496.23635848798, 895.0327043563617,\n",
      "      1737.0278599244639, 767.953674524085, 633.6234196359142, 652.4730178949907,\n",
      "      849.3171669157824, 4159.39080331878, 1037.736922412328, 2120.204144741036, 678.6962218624631,\n",
      "      1070.3890477315838, 4946.780673299452, 4664.239975562492, 833.7270931364699,\n",
      "      577.5339373958607, 743.9027535688547, 861.7732978345352, 708.2866832133049,\n",
      "      424.589839972002, 1236.916005369837, 3807.2755978690293, 5927.717844719117,\n",
      "      710.6624097889007, 750.2252321690175, 941.6793370935457, 1984.8599196166913,\n",
      "      864.0606285807639, 5157.960534492808, 634.95916097689, 946.1144582021124, 780.7794137626706,\n",
      "      792.0507646607382, 970.5188310342165, 867.8035982955956, 934.2583384399143,\n",
      "      1951.0770615797605, 2226.6292573145356, 710.1163215565508, 868.7472248567807,\n",
      "      1987.6190789687637, 4191.063345607012, 1019.170098030223, 831.9898087230031,\n",
      "      2649.5322602801607, 5208.094057022858, 1111.9934148737343, 581.2675694524054,\n",
      "      4662.96465402081, 949.2171694285682, 875.5598724564888, 471.5504952451999, 1054.8089181482121,\n",
      "      1675.193115630187, 3274.3443119585268, 593.8689433323682, 754.9063993762035,\n",
      "      5343.633322842861, 531.5996071923092, 464.64311097687596, 1253.1062020120225,\n",
      "      1900.4163968126363, 852.8060520774759, 5531.089190748678, 2459.5343035239707,\n",
      "      327.7809400161942, 5863.981917098787, 3104.490207572946, 698.3069915695949,\n",
      "      1745.01643841091, 825.4183470144234, 2059.6122634588846, 3349.1676767353933,\n",
      "      534.4913543496901, 2372.742404710204, 383.93894866220325, 1052.1882197584584,\n",
      "      3834.7490499648466, 669.5019607786002, 2498.7253867019845, 660.6308608422474]\n",
      "  num_episodes: 66\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.13610573541547277\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9881465775213158\n",
      "    mean_inference_ms: 0.9647670604893597\n",
      "    mean_raw_obs_processing_ms: 0.5754526951280922\n",
      "episode_media: {}\n",
      "hostname: Brandons-PC\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.10000000000000002\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.9914055849916191\n",
      "        entropy_coeff: 0.0\n",
      "        grad_gnorm: 1.4105353549603492\n",
      "        kl: 0.009141709095925888\n",
      "        policy_loss: -0.02401082389937934\n",
      "        total_loss: 9.974562231699625\n",
      "        vf_explained_var: 1.6525868446596207e-05\n",
      "        vf_loss: 9.997658864913449\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "      num_grad_updates_lifetime: 7905.5\n",
      "  num_agent_steps_sampled: 36000\n",
      "  num_agent_steps_trained: 36000\n",
      "  num_env_steps_sampled: 36000\n",
      "  num_env_steps_trained: 36000\n",
      "iterations_since_restore: 9\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 36000\n",
      "num_agent_steps_sampled_lifetime: 36000\n",
      "num_agent_steps_trained: 36000\n",
      "num_env_steps_sampled: 36000\n",
      "num_env_steps_sampled_lifetime: 36000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_sampled_throughput_per_sec: 181.88584101734293\n",
      "num_env_steps_trained: 36000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_env_steps_trained_throughput_per_sec: 181.88584101734293\n",
      "num_healthy_workers: 2\n",
      "num_in_flight_async_sample_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 22.787096774193547\n",
      "  ram_util_percent: 87.41290322580646\n",
      "pid: 52792\n",
      "time_since_restore: 230.83789682388306\n",
      "time_this_iter_s: 21.9998140335083\n",
      "time_total_s: 230.83789682388306\n",
      "timers:\n",
      "  learn_throughput: 197.641\n",
      "  learn_time_ms: 20238.676\n",
      "  load_throughput: 14247494.244\n",
      "  load_time_ms: 0.281\n",
      "  restore_workers_time_ms: 0.223\n",
      "  sample_time_ms: 5387.666\n",
      "  synch_weights_time_ms: 11.672\n",
      "  training_iteration_time_ms: 25639.742\n",
      "  training_step_time_ms: 25639.409\n",
      "timestamp: 1720197488\n",
      "timesteps_total: 36000\n",
      "training_iteration: 9\n",
      "trial_id: default\n",
      "\n",
      "Training episode 9\n",
      "agent_timesteps_total: 40000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 40000\n",
      "  num_agent_steps_trained: 40000\n",
      "  num_env_steps_sampled: 40000\n",
      "  num_env_steps_trained: 40000\n",
      "custom_metrics: {}\n",
      "date: 2024-07-05_11-38-34\n",
      "done: false\n",
      "env_runners:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.00500178337097168\n",
      "    StateBufferConnector_ms: 0.0039997100830078125\n",
      "    ViewRequirementAgentConnector_ms: 0.12441635131835938\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 61.0\n",
      "  episode_media: {}\n",
      "  episode_return_max: 6587.525039257994\n",
      "  episode_return_mean: 1819.4251240036285\n",
      "  episode_return_min: 327.7809400161942\n",
      "  episode_reward_max: 6587.525039257994\n",
      "  episode_reward_mean: 1819.4251240036285\n",
      "  episode_reward_min: 327.7809400161942\n",
      "  episodes_this_iter: 64\n",
      "  episodes_timesteps_total: 6100\n",
      "  hist_stats:\n",
      "    episode_lengths: [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61]\n",
      "    episode_reward: [5208.094057022858, 1111.9934148737343, 581.2675694524054, 4662.96465402081,\n",
      "      949.2171694285682, 875.5598724564888, 471.5504952451999, 1054.8089181482121,\n",
      "      1675.193115630187, 3274.3443119585268, 593.8689433323682, 754.9063993762035,\n",
      "      5343.633322842861, 531.5996071923092, 464.64311097687596, 1253.1062020120225,\n",
      "      1900.4163968126363, 852.8060520774759, 5531.089190748678, 2459.5343035239707,\n",
      "      327.7809400161942, 5863.981917098787, 3104.490207572946, 698.3069915695949,\n",
      "      1745.01643841091, 825.4183470144234, 2059.6122634588846, 3349.1676767353933,\n",
      "      534.4913543496901, 2372.742404710204, 383.93894866220325, 1052.1882197584584,\n",
      "      3834.7490499648466, 669.5019607786002, 2498.7253867019845, 660.6308608422474,\n",
      "      1368.3877027706212, 4161.704007235615, 3132.1338857783376, 1070.5009134584027,\n",
      "      6587.525039257994, 5929.604741852962, 706.7259018433358, 811.9647096231143,\n",
      "      465.18307810170546, 743.2783384926149, 680.1689066397182, 1180.5988322031803,\n",
      "      2520.7252881897616, 814.0582370400698, 5547.044200277635, 573.3815670128043,\n",
      "      859.0336955058019, 4104.705179345157, 770.6521898192582, 573.7519281975061,\n",
      "      2046.9325872527015, 3494.0428399346038, 695.4937836215244, 691.97963577695,\n",
      "      2366.3706305423384, 1590.5216236517938, 644.2411293043658, 469.76917209951637,\n",
      "      1539.657659799022, 826.7432161926691, 589.5929690018957, 583.0555838318686,\n",
      "      1926.9213399707935, 2990.072053168788, 5817.546723262096, 706.4570130309812,\n",
      "      3060.329859265946, 555.6508269788525, 2729.7040673198567, 605.559520095874,\n",
      "      727.4530551523204, 1098.6973528620301, 615.7257551724205, 616.487285176084,\n",
      "      710.5011571265452, 685.2872768496803, 1366.7914642001524, 478.57760929456646,\n",
      "      1579.8575412414966, 620.7712268348887, 4599.103279497101, 946.3800114084415,\n",
      "      617.6631614942656, 854.512881156239, 4563.4974999183505, 1161.1044531804018,\n",
      "      1384.716289189976, 887.8627397963805, 388.267243853857, 728.8332774766513, 2963.0125599768444,\n",
      "      767.1734496140496, 2425.055180503223, 5092.06999786107]\n",
      "  num_episodes: 64\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.13633161447340858\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.9877102808059527\n",
      "    mean_inference_ms: 0.9605408859398017\n",
      "    mean_raw_obs_processing_ms: 0.5739450812383091\n",
      "episode_media: {}\n",
      "hostname: Brandons-PC\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.10000000000000002\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.9830735992359859\n",
      "        entropy_coeff: 0.0\n",
      "        grad_gnorm: 1.63594442051585\n",
      "        kl: 0.010870446959531264\n",
      "        policy_loss: -0.027561511118866264\n",
      "        total_loss: 9.973525524139404\n",
      "        vf_explained_var: 9.054509542321646e-05\n",
      "        vf_loss: 10.0\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "      num_grad_updates_lifetime: 8835.5\n",
      "  num_agent_steps_sampled: 40000\n",
      "  num_agent_steps_trained: 40000\n",
      "  num_env_steps_sampled: 40000\n",
      "  num_env_steps_trained: 40000\n",
      "iterations_since_restore: 10\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 40000\n",
      "num_agent_steps_sampled_lifetime: 40000\n",
      "num_agent_steps_trained: 40000\n",
      "num_env_steps_sampled: 40000\n",
      "num_env_steps_sampled_lifetime: 40000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_sampled_throughput_per_sec: 153.8719793359847\n",
      "num_env_steps_trained: 40000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_env_steps_trained_throughput_per_sec: 153.8719793359847\n",
      "num_healthy_workers: 2\n",
      "num_in_flight_async_sample_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 15.758333333333335\n",
      "  ram_util_percent: 87.625\n",
      "pid: 52792\n",
      "time_since_restore: 256.8405556678772\n",
      "time_this_iter_s: 26.00265884399414\n",
      "time_total_s: 256.8405556678772\n",
      "timers:\n",
      "  learn_throughput: 197.155\n",
      "  learn_time_ms: 20288.644\n",
      "  load_throughput: 11342854.439\n",
      "  load_time_ms: 0.353\n",
      "  restore_workers_time_ms: 0.2\n",
      "  sample_time_ms: 5373.7\n",
      "  synch_weights_time_ms: 11.233\n",
      "  training_iteration_time_ms: 25675.332\n",
      "  training_step_time_ms: 25675.032\n",
      "timestamp: 1720197514\n",
      "timesteps_total: 40000\n",
      "training_iteration: 10\n",
      "trial_id: default\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray.rllib.algorithms import ppo\n",
    "from ray.tune.logger import pretty_print\n",
    "import trading_env\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init()\n",
    "\n",
    "algo = ppo.PPO(env=trading_env.TradingEnv, config={\"env_config\": {}})\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"Training episode {i}\")\n",
    "    result = algo.train()\n",
    "    print(pretty_print(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved in directory C:\\Users\\brand\\AppData\\Local\\Temp\\tmpkap_4ue5\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = algo.save().checkpoint.path\n",
    "print(f\"Checkpoint saved in directory {checkpoint_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
