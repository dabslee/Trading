{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trader Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-08 09:59:57,156\tINFO worker.py:1771 -- Started a local Ray instance.\n",
      "2024-07-08 10:00:03,775\tWARNING deprecation.py:50 -- DeprecationWarning: `algo = Algorithm(env='<class 'trading_env.TradingEnv'>', ...)` has been deprecated. Use `algo = AlgorithmConfig().environment('<class 'trading_env.TradingEnv'>').build()` instead. This will raise an error in the future!\n",
      "c:\\Users\\brand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py:516: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "c:\\Users\\brand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ray\\tune\\logger\\unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "c:\\Users\\brand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ray\\tune\\logger\\unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "c:\\Users\\brand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ray\\tune\\logger\\unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "\u001b[36m(pid=25192)\u001b[0m WARNING:tensorflow:From c:\\Users\\brand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ray\\rllib\\utils\\framework.py:126: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\u001b[36m(pid=25192)\u001b[0m \n",
      "\u001b[36m(pid=7700)\u001b[0m \n",
      "2024-07-08 10:00:31,024\tINFO trainable.py:161 -- Trainable.setup took 27.232 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2024-07-08 10:00:31,027\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training episode 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-08 10:00:36,660\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.multi_gpu_train_one_step` has been deprecated. This will raise an error in the future!\n",
      "2024-07-08 10:00:52,634\tWARNING ppo.py:696 -- The mean reward returned from the environment is 38.98219680786133 but the vf_clip_param is set to 10.0. Consider increasing it for policy: default_policy to improve value function convergence.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 4000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 4000\n",
      "  num_agent_steps_trained: 4000\n",
      "  num_env_steps_sampled: 4000\n",
      "  num_env_steps_trained: 4000\n",
      "custom_metrics: {}\n",
      "date: 2024-07-08_10-00-52\n",
      "done: false\n",
      "env_runners:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0032044947147369385\n",
      "    StateBufferConnector_ms: 0.002501159906387329\n",
      "    ViewRequirementAgentConnector_ms: 0.11257082223892212\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 61.0\n",
      "  episode_media: {}\n",
      "  episode_return_max: 7922.130877484104\n",
      "  episode_return_mean: 2450.2548234967267\n",
      "  episode_return_min: 540.8529828587009\n",
      "  episode_reward_max: 7922.130877484104\n",
      "  episode_reward_mean: 2450.2548234967267\n",
      "  episode_reward_min: 540.8529828587009\n",
      "  episodes_this_iter: 64\n",
      "  episodes_timesteps_total: 3904\n",
      "  hist_stats:\n",
      "    episode_lengths: [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61]\n",
      "    episode_reward: [813.3876335294144, 3146.6515688576815, 1113.4034272603235, 3125.5330533614,\n",
      "      879.6351615628598, 985.2201395901291, 1691.4941313291758, 6538.909576079061,\n",
      "      4400.817519557902, 865.6261452150095, 1686.0603664725163, 701.7291795030802,\n",
      "      6406.716946387098, 624.4595527354062, 7026.396243041901, 1962.7452673955827,\n",
      "      625.1265487454738, 590.5397173586787, 2140.3187860564713, 927.2084348622764,\n",
      "      731.8406848553356, 6062.152358564411, 1067.2658989834329, 983.9658733701531,\n",
      "      2144.6040297117966, 896.518979463849, 7635.7860261888845, 1023.5330287124995,\n",
      "      7922.130877484104, 989.3124562666592, 794.1699028859608, 1650.3939681973304,\n",
      "      808.4892902232782, 1030.628030023347, 643.172160669785, 1145.923418273832, 540.8529828587009,\n",
      "      3254.35525796449, 608.6116066717645, 2208.9073569425377, 5820.701825171588,\n",
      "      2101.4395557899616, 845.5328063286318, 2090.2201887309407, 989.1656451264405,\n",
      "      4537.572402972465, 947.545905039517, 839.0146552519479, 6814.708815208923, 1032.0836389645972,\n",
      "      1481.2276153564064, 1269.9706695922623, 889.6421874325871, 1961.4610590175541,\n",
      "      1037.4838272921636, 6214.239002030879, 2380.279590744777, 911.2125616403952,\n",
      "      6135.145872598727, 686.3372507639094, 7648.143918945186, 2184.098340095019,\n",
      "      1933.7337513072337, 7670.7540311748]\n",
      "  num_episodes: 64\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.137115525698912\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0401584219181914\n",
      "    mean_inference_ms: 1.012031046644799\n",
      "    mean_raw_obs_processing_ms: 0.5879209138106728\n",
      "episode_media: {}\n",
      "hostname: Brandons-PC\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.3963331633998501\n",
      "        entropy_coeff: 0.0\n",
      "        grad_gnorm: 1.1203765944447568\n",
      "        kl: 0.003984378319172944\n",
      "        policy_loss: -0.01736180236042347\n",
      "        total_loss: 9.983435057568293\n",
      "        vf_explained_var: 3.6147332960559475e-07\n",
      "        vf_loss: 10.0\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "      num_grad_updates_lifetime: 465.5\n",
      "  num_agent_steps_sampled: 4000\n",
      "  num_agent_steps_trained: 4000\n",
      "  num_env_steps_sampled: 4000\n",
      "  num_env_steps_trained: 4000\n",
      "iterations_since_restore: 1\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 4000\n",
      "num_agent_steps_sampled_lifetime: 4000\n",
      "num_agent_steps_trained: 4000\n",
      "num_env_steps_sampled: 4000\n",
      "num_env_steps_sampled_lifetime: 4000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_sampled_throughput_per_sec: 185.2241784292313\n",
      "num_env_steps_trained: 4000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_env_steps_trained_throughput_per_sec: 185.2241784292313\n",
      "num_healthy_workers: 2\n",
      "num_in_flight_async_sample_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 18.44516129032258\n",
      "  ram_util_percent: 88.22258064516129\n",
      "pid: 18244\n",
      "time_since_restore: 21.604016065597534\n",
      "time_this_iter_s: 21.604016065597534\n",
      "time_total_s: 21.604016065597534\n",
      "timers:\n",
      "  learn_throughput: 250.678\n",
      "  learn_time_ms: 15956.727\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  restore_workers_time_ms: 0.0\n",
      "  sample_time_ms: 5619.718\n",
      "  synch_weights_time_ms: 16.575\n",
      "  training_iteration_time_ms: 21595.453\n",
      "  training_step_time_ms: 21595.453\n",
      "timestamp: 1720450852\n",
      "timesteps_total: 4000\n",
      "training_iteration: 1\n",
      "trial_id: default\n",
      "\n",
      "Training episode 1\n",
      "agent_timesteps_total: 8000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 8000\n",
      "  num_agent_steps_trained: 8000\n",
      "  num_env_steps_sampled: 8000\n",
      "  num_env_steps_trained: 8000\n",
      "custom_metrics: {}\n",
      "date: 2024-07-08_10-01-15\n",
      "done: false\n",
      "env_runners:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.003952503204345703\n",
      "    StateBufferConnector_ms: 0.002655506134033203\n",
      "    ViewRequirementAgentConnector_ms: 0.12059426307678223\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 61.0\n",
      "  episode_media: {}\n",
      "  episode_return_max: 7670.7540311748\n",
      "  episode_return_mean: 2243.8371369708234\n",
      "  episode_return_min: 495.90650679437243\n",
      "  episode_reward_max: 7670.7540311748\n",
      "  episode_reward_mean: 2243.8371369708234\n",
      "  episode_reward_min: 495.90650679437243\n",
      "  episodes_this_iter: 66\n",
      "  episodes_timesteps_total: 6100\n",
      "  hist_stats:\n",
      "    episode_lengths: [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61]\n",
      "    episode_reward: [794.1699028859608, 1650.3939681973304, 808.4892902232782, 1030.628030023347,\n",
      "      643.172160669785, 1145.923418273832, 540.8529828587009, 3254.35525796449, 608.6116066717645,\n",
      "      2208.9073569425377, 5820.701825171588, 2101.4395557899616, 845.5328063286318,\n",
      "      2090.2201887309407, 989.1656451264405, 4537.572402972465, 947.545905039517,\n",
      "      839.0146552519479, 6814.708815208923, 1032.0836389645972, 1481.2276153564064,\n",
      "      1269.9706695922623, 889.6421874325871, 1961.4610590175541, 1037.4838272921636,\n",
      "      6214.239002030879, 2380.279590744777, 911.2125616403952, 6135.145872598727,\n",
      "      686.3372507639094, 7648.143918945186, 2184.098340095019, 1933.7337513072337,\n",
      "      7670.7540311748, 1005.835600691096, 1283.7561545503845, 2167.4628492802085,\n",
      "      1900.6090391794078, 802.0757903518447, 1232.3136231446983, 3205.204558819924,\n",
      "      2630.4910632762476, 2628.4534535559806, 859.1394666115394, 1669.8031491605966,\n",
      "      659.6106091094443, 6651.0488105085315, 715.3065889918461, 1465.0096021838242,\n",
      "      495.90650679437243, 1120.4110273570236, 821.0396042121188, 1784.9078114833096,\n",
      "      927.679704122665, 560.6068827359207, 671.5601955795405, 858.325665445089, 1678.2140724976757,\n",
      "      674.8979463694366, 2360.364601266751, 4874.607521423699, 898.7273858885882,\n",
      "      2268.016312786639, 3972.1193402687913, 932.2959422289107, 2306.189331049748,\n",
      "      519.6791334429039, 1806.537167902412, 2865.8681585698278, 5641.195053374752,\n",
      "      1649.7758851795459, 966.3627538213686, 916.6318311112435, 5184.157019876366,\n",
      "      946.2097734278224, 1096.6463868763512, 1221.0539785739884, 1311.8031688043286,\n",
      "      5840.533811527433, 1564.6316320210697, 1018.9001382269842, 1172.4966692670357,\n",
      "      837.792530494221, 4025.1468404311563, 2591.039087465736, 3720.003150054838,\n",
      "      636.3264784742042, 692.5853551262862, 1454.7713458977516, 7647.899886208623,\n",
      "      5325.2104542804145, 1597.8730837297737, 1274.6268807325916, 6892.815697292449,\n",
      "      4008.147893579451, 2093.6876187274574, 833.2825408663483, 6061.5831187330205,\n",
      "      2901.5605527923717, 877.669317978447]\n",
      "  num_episodes: 66\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.13849825566887086\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0782741627373449\n",
      "    mean_inference_ms: 1.041828425601065\n",
      "    mean_raw_obs_processing_ms: 0.6132943523501042\n",
      "episode_media: {}\n",
      "hostname: Brandons-PC\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.10000000000000002\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.3499829861425585\n",
      "        entropy_coeff: 0.0\n",
      "        grad_gnorm: 1.1516490539555908\n",
      "        kl: 0.00962738956943521\n",
      "        policy_loss: -0.0247217588967854\n",
      "        total_loss: 9.976240951784195\n",
      "        vf_explained_var: 4.1146432199785787e-07\n",
      "        vf_loss: 10.0\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "      num_grad_updates_lifetime: 1395.5\n",
      "  num_agent_steps_sampled: 8000\n",
      "  num_agent_steps_trained: 8000\n",
      "  num_env_steps_sampled: 8000\n",
      "  num_env_steps_trained: 8000\n",
      "iterations_since_restore: 2\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 8000\n",
      "num_agent_steps_sampled_lifetime: 8000\n",
      "num_agent_steps_trained: 8000\n",
      "num_env_steps_sampled: 8000\n",
      "num_env_steps_sampled_lifetime: 8000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_sampled_throughput_per_sec: 172.71986385769054\n",
      "num_env_steps_trained: 8000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_env_steps_trained_throughput_per_sec: 172.71986385769054\n",
      "num_healthy_workers: 2\n",
      "num_in_flight_async_sample_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 27.900000000000002\n",
      "  ram_util_percent: 88.359375\n",
      "pid: 18244\n",
      "time_since_restore: 44.77146863937378\n",
      "time_this_iter_s: 23.167452573776245\n",
      "time_total_s: 44.77146863937378\n",
      "timers:\n",
      "  learn_throughput: 242.836\n",
      "  learn_time_ms: 16472.027\n",
      "  load_throughput: 7996766.444\n",
      "  load_time_ms: 0.5\n",
      "  restore_workers_time_ms: 0.0\n",
      "  sample_time_ms: 5889.125\n",
      "  synch_weights_time_ms: 14.302\n",
      "  training_iteration_time_ms: 22377.17\n",
      "  training_step_time_ms: 22377.17\n",
      "timestamp: 1720450875\n",
      "timesteps_total: 8000\n",
      "training_iteration: 2\n",
      "trial_id: default\n",
      "\n",
      "Training episode 2\n",
      "agent_timesteps_total: 12000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 12000\n",
      "  num_agent_steps_trained: 12000\n",
      "  num_env_steps_sampled: 12000\n",
      "  num_env_steps_trained: 12000\n",
      "custom_metrics: {}\n",
      "date: 2024-07-08_10-01-38\n",
      "done: false\n",
      "env_runners:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.003683328628540039\n",
      "    StateBufferConnector_ms: 0.00099945068359375\n",
      "    ViewRequirementAgentConnector_ms: 0.13480663299560547\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 61.0\n",
      "  episode_media: {}\n",
      "  episode_return_max: 7647.899886208623\n",
      "  episode_return_mean: 2253.333418010302\n",
      "  episode_return_min: 472.24732587988876\n",
      "  episode_reward_max: 7647.899886208623\n",
      "  episode_reward_mean: 2253.333418010302\n",
      "  episode_reward_min: 472.24732587988876\n",
      "  episodes_this_iter: 66\n",
      "  episodes_timesteps_total: 6100\n",
      "  hist_stats:\n",
      "    episode_lengths: [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61]\n",
      "    episode_reward: [519.6791334429039, 1806.537167902412, 2865.8681585698278, 5641.195053374752,\n",
      "      1649.7758851795459, 966.3627538213686, 916.6318311112435, 5184.157019876366,\n",
      "      946.2097734278224, 1096.6463868763512, 1221.0539785739884, 1311.8031688043286,\n",
      "      5840.533811527433, 1564.6316320210697, 1018.9001382269842, 1172.4966692670357,\n",
      "      837.792530494221, 4025.1468404311563, 2591.039087465736, 3720.003150054838,\n",
      "      636.3264784742042, 692.5853551262862, 1454.7713458977516, 7647.899886208623,\n",
      "      5325.2104542804145, 1597.8730837297737, 1274.6268807325916, 6892.815697292449,\n",
      "      4008.147893579451, 2093.6876187274574, 833.2825408663483, 6061.5831187330205,\n",
      "      2901.5605527923717, 877.669317978447, 1975.4652088939888, 1268.6079788182467,\n",
      "      878.2996574501713, 721.6738988486226, 1957.5241450999788, 693.6678827615531,\n",
      "      5172.037618950913, 3414.5744116928686, 3218.715348107792, 3648.368705946349,\n",
      "      4529.234610400739, 1300.5847585714555, 661.1789615736058, 640.2629650954729,\n",
      "      1140.946941266622, 4912.766970928329, 1155.1454382389866, 709.0009882277237,\n",
      "      6566.178140561118, 909.477175268466, 3699.9825465294134, 772.7514650090411,\n",
      "      1041.815466391622, 4250.992181066328, 5566.776970329116, 2124.411241643459,\n",
      "      915.0832715788591, 1029.770938977391, 832.502523839783, 3872.4752637250726,\n",
      "      599.6388055758136, 1003.8284944462874, 895.5463865320693, 923.5963307906022,\n",
      "      775.7216746819943, 5188.26017676965, 940.188739516247, 703.5479264553934, 550.9865046355901,\n",
      "      1771.3068486612015, 1038.375170364743, 472.24732587988876, 4883.1396271996755,\n",
      "      1669.3224026041853, 2418.087504750753, 1731.1506710959197, 6075.737829723551,\n",
      "      1317.0743491009189, 603.1009316873075, 4025.623002045456, 2948.449008167859,\n",
      "      745.2569665000185, 1251.4404413496618, 974.3794065531856, 588.3177466045972,\n",
      "      2115.3491059979056, 871.1416204039439, 1804.2747270607242, 776.0667615517057,\n",
      "      3766.2196782334718, 3178.793951160334, 6445.517341919258, 841.3622024859583,\n",
      "      587.8867193348435, 2987.900164226129, 1089.7271863016533]\n",
      "  num_episodes: 66\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14519969605464467\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.0977757943994708\n",
      "    mean_inference_ms: 1.0598502996160075\n",
      "    mean_raw_obs_processing_ms: 0.6377749634999319\n",
      "episode_media: {}\n",
      "hostname: Brandons-PC\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.10000000000000002\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.308689473393143\n",
      "        entropy_coeff: 0.0\n",
      "        grad_gnorm: 1.055223483936761\n",
      "        kl: 0.00768650306522774\n",
      "        policy_loss: -0.023582632841682563\n",
      "        total_loss: 9.977186018420804\n",
      "        vf_explained_var: 2.0188670004567793e-07\n",
      "        vf_loss: 10.0\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "      num_grad_updates_lifetime: 2325.5\n",
      "  num_agent_steps_sampled: 12000\n",
      "  num_agent_steps_trained: 12000\n",
      "  num_env_steps_sampled: 12000\n",
      "  num_env_steps_trained: 12000\n",
      "iterations_since_restore: 3\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 12000\n",
      "num_agent_steps_sampled_lifetime: 12000\n",
      "num_agent_steps_trained: 12000\n",
      "num_env_steps_sampled: 12000\n",
      "num_env_steps_sampled_lifetime: 12000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_sampled_throughput_per_sec: 174.30808932679503\n",
      "num_env_steps_trained: 12000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_env_steps_trained_throughput_per_sec: 174.30808932679503\n",
      "num_healthy_workers: 2\n",
      "num_in_flight_async_sample_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 28.28181818181818\n",
      "  ram_util_percent: 88.41515151515152\n",
      "pid: 18244\n",
      "time_since_restore: 67.72786569595337\n",
      "time_this_iter_s: 22.95639705657959\n",
      "time_total_s: 67.72786569595337\n",
      "timers:\n",
      "  learn_throughput: 240.921\n",
      "  learn_time_ms: 16602.952\n",
      "  load_throughput: 11995149.666\n",
      "  load_time_ms: 0.333\n",
      "  restore_workers_time_ms: 0.0\n",
      "  sample_time_ms: 5950.928\n",
      "  synch_weights_time_ms: 12.047\n",
      "  training_iteration_time_ms: 22567.405\n",
      "  training_step_time_ms: 22567.405\n",
      "timestamp: 1720450898\n",
      "timesteps_total: 12000\n",
      "training_iteration: 3\n",
      "trial_id: default\n",
      "\n",
      "Training episode 3\n",
      "agent_timesteps_total: 16000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 16000\n",
      "  num_agent_steps_trained: 16000\n",
      "  num_env_steps_sampled: 16000\n",
      "  num_env_steps_trained: 16000\n",
      "custom_metrics: {}\n",
      "date: 2024-07-08_10-02-13\n",
      "done: false\n",
      "env_runners:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.009159326553344727\n",
      "    StateBufferConnector_ms: 0.00301361083984375\n",
      "    ViewRequirementAgentConnector_ms: 0.2447371482849121\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 61.0\n",
      "  episode_media: {}\n",
      "  episode_return_max: 7362.0032862940725\n",
      "  episode_return_mean: 1923.7544968988545\n",
      "  episode_return_min: 393.2262614862529\n",
      "  episode_reward_max: 7362.0032862940725\n",
      "  episode_reward_mean: 1923.7544968988545\n",
      "  episode_reward_min: 393.2262614862529\n",
      "  episodes_this_iter: 66\n",
      "  episodes_timesteps_total: 6100\n",
      "  hist_stats:\n",
      "    episode_lengths: [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61]\n",
      "    episode_reward: [895.5463865320693, 923.5963307906022, 775.7216746819943, 5188.26017676965,\n",
      "      940.188739516247, 703.5479264553934, 550.9865046355901, 1771.3068486612015,\n",
      "      1038.375170364743, 472.24732587988876, 4883.1396271996755, 1669.3224026041853,\n",
      "      2418.087504750753, 1731.1506710959197, 6075.737829723551, 1317.0743491009189,\n",
      "      603.1009316873075, 4025.623002045456, 2948.449008167859, 745.2569665000185,\n",
      "      1251.4404413496618, 974.3794065531856, 588.3177466045972, 2115.3491059979056,\n",
      "      871.1416204039439, 1804.2747270607242, 776.0667615517057, 3766.2196782334718,\n",
      "      3178.793951160334, 6445.517341919258, 841.3622024859583, 587.8867193348435,\n",
      "      2987.900164226129, 1089.7271863016533, 1254.1457880297528, 2023.119612470195,\n",
      "      1236.0464938590458, 2651.791072311699, 1808.223002386093, 5548.0803752615375,\n",
      "      2050.8108454161884, 767.1490087299544, 1269.4713254079998, 713.3348203454633,\n",
      "      3990.5790589980743, 2505.749154089621, 1061.0126473263056, 2128.706184851429,\n",
      "      2009.7940595195448, 613.3788299116055, 1177.817947847453, 736.8287632935044,\n",
      "      6044.852584911554, 3768.51890858639, 490.8570862721119, 713.4205704753813, 605.4642375848912,\n",
      "      6547.764795089894, 800.3199762329142, 1087.6052064603891, 1086.5875021855668,\n",
      "      782.0418233729018, 606.6348418607829, 393.2262614862529, 1260.3756332239138,\n",
      "      534.7944453078147, 2401.147237113405, 960.4350980549285, 1521.1034776474141,\n",
      "      1053.0764261990716, 501.4590367705188, 750.3468383637942, 2530.107906131345,\n",
      "      715.4062188584462, 1144.099369906429, 1613.8402540674806, 6117.937696090727,\n",
      "      5269.739612144257, 416.72420103750824, 1625.2726933826775, 535.8240099402035,\n",
      "      6706.028290496746, 972.6263007955066, 796.836162791711, 3849.4652413550825,\n",
      "      712.1365257851102, 1007.6267071252184, 550.194184192606, 842.0719353734878,\n",
      "      698.9514876408319, 1848.4441981369143, 3570.7651795946567, 846.9396536214957,\n",
      "      7362.0032862940725, 4391.273165554805, 2744.725546558202, 989.3231066325851,\n",
      "      969.9102288469912, 688.0133186883511, 447.9958012402252]\n",
      "  num_episodes: 66\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18993619769770578\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.398278074111917\n",
      "    mean_inference_ms: 1.345648862718764\n",
      "    mean_raw_obs_processing_ms: 0.7903130549639422\n",
      "episode_media: {}\n",
      "hostname: Brandons-PC\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.10000000000000002\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.228361234100916\n",
      "        entropy_coeff: 0.0\n",
      "        grad_gnorm: 1.1200549053248539\n",
      "        kl: 0.012577373724895057\n",
      "        policy_loss: -0.024508011523604153\n",
      "        total_loss: 9.976749709344679\n",
      "        vf_explained_var: 7.306375811176915e-07\n",
      "        vf_loss: 10.0\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "      num_grad_updates_lifetime: 3255.5\n",
      "  num_agent_steps_sampled: 16000\n",
      "  num_agent_steps_trained: 16000\n",
      "  num_env_steps_sampled: 16000\n",
      "  num_env_steps_trained: 16000\n",
      "iterations_since_restore: 4\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 16000\n",
      "num_agent_steps_sampled_lifetime: 16000\n",
      "num_agent_steps_trained: 16000\n",
      "num_env_steps_sampled: 16000\n",
      "num_env_steps_sampled_lifetime: 16000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_sampled_throughput_per_sec: 116.82437975388052\n",
      "num_env_steps_trained: 16000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_env_steps_trained_throughput_per_sec: 116.82437975388052\n",
      "num_healthy_workers: 2\n",
      "num_in_flight_async_sample_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 35.13478260869565\n",
      "  ram_util_percent: 90.3804347826087\n",
      "pid: 18244\n",
      "time_since_restore: 101.9748330116272\n",
      "time_this_iter_s: 34.24696731567383\n",
      "time_total_s: 101.9748330116272\n",
      "timers:\n",
      "  learn_throughput: 232.991\n",
      "  learn_time_ms: 17168.035\n",
      "  load_throughput: 15993532.888\n",
      "  load_time_ms: 0.25\n",
      "  restore_workers_time_ms: 0.0\n",
      "  sample_time_ms: 8304.836\n",
      "  synch_weights_time_ms: 10.929\n",
      "  training_iteration_time_ms: 25485.411\n",
      "  training_step_time_ms: 25485.411\n",
      "timestamp: 1720450933\n",
      "timesteps_total: 16000\n",
      "training_iteration: 4\n",
      "trial_id: default\n",
      "\n",
      "Training episode 4\n",
      "agent_timesteps_total: 20000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 20000\n",
      "  num_agent_steps_trained: 20000\n",
      "  num_env_steps_sampled: 20000\n",
      "  num_env_steps_trained: 20000\n",
      "custom_metrics: {}\n",
      "date: 2024-07-08_10-02-34\n",
      "done: false\n",
      "env_runners:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.00843501091003418\n",
      "    StateBufferConnector_ms: 0.001979351043701172\n",
      "    ViewRequirementAgentConnector_ms: 0.16645193099975586\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 61.0\n",
      "  episode_media: {}\n",
      "  episode_return_max: 7362.0032862940725\n",
      "  episode_return_mean: 1717.3352640240948\n",
      "  episode_return_min: 412.32869084579295\n",
      "  episode_reward_max: 7362.0032862940725\n",
      "  episode_reward_mean: 1717.3352640240948\n",
      "  episode_reward_min: 412.32869084579295\n",
      "  episodes_this_iter: 64\n",
      "  episodes_timesteps_total: 6100\n",
      "  hist_stats:\n",
      "    episode_lengths: [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61]\n",
      "    episode_reward: [1260.3756332239138, 534.7944453078147, 2401.147237113405, 960.4350980549285,\n",
      "      1521.1034776474141, 1053.0764261990716, 501.4590367705188, 750.3468383637942,\n",
      "      2530.107906131345, 715.4062188584462, 1144.099369906429, 1613.8402540674806,\n",
      "      6117.937696090727, 5269.739612144257, 416.72420103750824, 1625.2726933826775,\n",
      "      535.8240099402035, 6706.028290496746, 972.6263007955066, 796.836162791711, 3849.4652413550825,\n",
      "      712.1365257851102, 1007.6267071252184, 550.194184192606, 842.0719353734878,\n",
      "      698.9514876408319, 1848.4441981369143, 3570.7651795946567, 846.9396536214957,\n",
      "      7362.0032862940725, 4391.273165554805, 2744.725546558202, 989.3231066325851,\n",
      "      969.9102288469912, 688.0133186883511, 447.9958012402252, 1581.9951570520204,\n",
      "      671.1752330425471, 835.3594334911139, 490.11153165077826, 1054.2141196097698,\n",
      "      4711.476416048317, 2060.0265644417505, 3123.6117290523216, 3638.6851788344734,\n",
      "      3510.1095996561917, 1386.564260500655, 4065.8851098382593, 1020.5380850578158,\n",
      "      587.8492834119085, 977.9628275041617, 607.8648704370314, 889.557279462349, 814.8903594641444,\n",
      "      1106.1425219073835, 617.9882014019585, 1170.269415635291, 847.7670480169234,\n",
      "      888.0811040923139, 1467.611505998591, 1394.7415028841788, 975.7822278687569,\n",
      "      441.07869208036016, 1390.465448249161, 7150.351268100674, 850.9519607353739,\n",
      "      637.3459358782152, 1227.5434718289446, 979.4009464229059, 827.0660712309584,\n",
      "      604.7688613202649, 1583.0159920967433, 993.9317633932174, 570.129771336542,\n",
      "      980.3383305427092, 412.32869084579295, 4204.687478775702, 575.8976008632843,\n",
      "      1944.9278305241467, 1327.6417511672328, 1012.8257006371218, 1249.6266637134527,\n",
      "      906.8030415283549, 918.0125828782182, 958.6193810559822, 1789.9855469173192,\n",
      "      2709.133507341209, 4942.432338502806, 699.2805966184343, 3179.273957594852,\n",
      "      667.127491989904, 880.6947540614713, 919.5845887252962, 971.2163828361173, 4544.05608691499,\n",
      "      1125.7591695998562, 5187.00208978373, 1445.9673740033002, 711.6387886062669,\n",
      "      769.3334523830126]\n",
      "  num_episodes: 64\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20500309692306756\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.4915965888865759\n",
      "    mean_inference_ms: 1.4208892287839747\n",
      "    mean_raw_obs_processing_ms: 0.836487938741511\n",
      "episode_media: {}\n",
      "hostname: Brandons-PC\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.10000000000000002\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.2179926371061673\n",
      "        entropy_coeff: 0.0\n",
      "        grad_gnorm: 1.1508325225243003\n",
      "        kl: 0.012463457296347461\n",
      "        policy_loss: -0.0277779877025594\n",
      "        total_loss: 9.960205955915553\n",
      "        vf_explained_var: 2.2657263663507277e-05\n",
      "        vf_loss: 9.986737590707758\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "      num_grad_updates_lifetime: 4185.5\n",
      "  num_agent_steps_sampled: 20000\n",
      "  num_agent_steps_trained: 20000\n",
      "  num_env_steps_sampled: 20000\n",
      "  num_env_steps_trained: 20000\n",
      "iterations_since_restore: 5\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 20000\n",
      "num_agent_steps_sampled_lifetime: 20000\n",
      "num_agent_steps_trained: 20000\n",
      "num_env_steps_sampled: 20000\n",
      "num_env_steps_sampled_lifetime: 20000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_sampled_throughput_per_sec: 183.08468370053632\n",
      "num_env_steps_trained: 20000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_env_steps_trained_throughput_per_sec: 183.08468370053632\n",
      "num_healthy_workers: 2\n",
      "num_in_flight_async_sample_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 24.61612903225806\n",
      "  ram_util_percent: 90.67741935483872\n",
      "pid: 18244\n",
      "time_since_restore: 123.83217024803162\n",
      "time_this_iter_s: 21.85733723640442\n",
      "time_total_s: 123.83217024803162\n",
      "timers:\n",
      "  learn_throughput: 236.169\n",
      "  learn_time_ms: 16937.052\n",
      "  load_throughput: 19991916.111\n",
      "  load_time_ms: 0.2\n",
      "  restore_workers_time_ms: 0.0\n",
      "  sample_time_ms: 7808.505\n",
      "  synch_weights_time_ms: 11.047\n",
      "  training_iteration_time_ms: 24757.891\n",
      "  training_step_time_ms: 24757.891\n",
      "timestamp: 1720450954\n",
      "timesteps_total: 20000\n",
      "training_iteration: 5\n",
      "trial_id: default\n",
      "\n",
      "Training episode 5\n",
      "agent_timesteps_total: 24000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 24000\n",
      "  num_agent_steps_trained: 24000\n",
      "  num_env_steps_sampled: 24000\n",
      "  num_env_steps_trained: 24000\n",
      "custom_metrics: {}\n",
      "date: 2024-07-08_10-03-00\n",
      "done: false\n",
      "env_runners:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.00815439224243164\n",
      "    StateBufferConnector_ms: 0.006913900375366211\n",
      "    ViewRequirementAgentConnector_ms: 0.10523867607116699\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 61.0\n",
      "  episode_media: {}\n",
      "  episode_return_max: 7259.727710266481\n",
      "  episode_return_mean: 1968.012926495044\n",
      "  episode_return_min: 412.32869084579295\n",
      "  episode_reward_max: 7259.727710266481\n",
      "  episode_reward_mean: 1968.012926495044\n",
      "  episode_reward_min: 412.32869084579295\n",
      "  episodes_this_iter: 66\n",
      "  episodes_timesteps_total: 6100\n",
      "  hist_stats:\n",
      "    episode_lengths: [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61]\n",
      "    episode_reward: [637.3459358782152, 1227.5434718289446, 979.4009464229059, 827.0660712309584,\n",
      "      604.7688613202649, 1583.0159920967433, 993.9317633932174, 570.129771336542,\n",
      "      980.3383305427092, 412.32869084579295, 4204.687478775702, 575.8976008632843,\n",
      "      1944.9278305241467, 1327.6417511672328, 1012.8257006371218, 1249.6266637134527,\n",
      "      906.8030415283549, 918.0125828782182, 958.6193810559822, 1789.9855469173192,\n",
      "      2709.133507341209, 4942.432338502806, 699.2805966184343, 3179.273957594852,\n",
      "      667.127491989904, 880.6947540614713, 919.5845887252962, 971.2163828361173, 4544.05608691499,\n",
      "      1125.7591695998562, 5187.00208978373, 1445.9673740033002, 711.6387886062669,\n",
      "      769.3334523830126, 791.3408699116004, 2003.5823566477, 7259.727710266481, 2937.7249859598896,\n",
      "      1261.0800123643824, 4830.915348906172, 912.9390363745608, 3395.0160411992383,\n",
      "      666.3733478286655, 3949.85004499486, 745.3397210694268, 6243.472061357279, 802.0407957914108,\n",
      "      2555.6913936415276, 1328.4084059231382, 512.9690281898621, 482.0929907456612,\n",
      "      1653.699641817869, 985.912155898048, 502.60298413503244, 567.0592061297504,\n",
      "      6373.506713617251, 531.2596680886513, 747.3645460877499, 1455.9816528874303,\n",
      "      1373.2225294127816, 1403.983450398854, 419.49996380650595, 5999.5841815599715,\n",
      "      3796.7800763806413, 585.6648260419828, 1248.4883545398088, 5798.161235066025,\n",
      "      1715.5066144469379, 925.8046898533228, 1002.4685594392093, 2516.865576071446,\n",
      "      1510.1861754598528, 3761.6483049912986, 4286.359980782743, 4173.482593625416,\n",
      "      5620.183036621868, 4526.875605410164, 707.1319231811335, 818.8082207922256,\n",
      "      1152.570564155667, 966.2589295578596, 6579.349349857072, 657.7316752713065,\n",
      "      608.1334200352945, 1244.4457206190975, 1364.1923848205324, 591.4623236905386,\n",
      "      529.7475996045661, 857.0964874930874, 5527.557107232951, 3546.1080963893055,\n",
      "      1420.2209645326434, 2298.8925284898864, 1505.155479514905, 2589.9733239807015,\n",
      "      442.83149476257057, 3127.913407534624, 1423.6238319213512, 1519.5934595538013,\n",
      "      704.3798908524744]\n",
      "  num_episodes: 66\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1954205807241828\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.4015782442688884\n",
      "    mean_inference_ms: 1.3389157857599676\n",
      "    mean_raw_obs_processing_ms: 0.7889837746390314\n",
      "episode_media: {}\n",
      "hostname: Brandons-PC\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.10000000000000002\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.2013043574107591\n",
      "        entropy_coeff: 0.0\n",
      "        grad_gnorm: 1.2228613284967278\n",
      "        kl: 0.012459047314087127\n",
      "        policy_loss: -0.02996425447404705\n",
      "        total_loss: 9.960975205001011\n",
      "        vf_explained_var: 3.8914206207439464e-05\n",
      "        vf_loss: 9.989693540142428\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "      num_grad_updates_lifetime: 5115.5\n",
      "  num_agent_steps_sampled: 24000\n",
      "  num_agent_steps_trained: 24000\n",
      "  num_env_steps_sampled: 24000\n",
      "  num_env_steps_trained: 24000\n",
      "iterations_since_restore: 6\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 24000\n",
      "num_agent_steps_sampled_lifetime: 24000\n",
      "num_agent_steps_trained: 24000\n",
      "num_env_steps_sampled: 24000\n",
      "num_env_steps_sampled_lifetime: 24000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_sampled_throughput_per_sec: 155.334232662242\n",
      "num_env_steps_trained: 24000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_env_steps_trained_throughput_per_sec: 155.334232662242\n",
      "num_healthy_workers: 2\n",
      "num_in_flight_async_sample_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 42.44444444444444\n",
      "  ram_util_percent: 90.18333333333334\n",
      "pid: 18244\n",
      "time_since_restore: 149.5935914516449\n",
      "time_this_iter_s: 25.76142120361328\n",
      "time_total_s: 149.5935914516449\n",
      "timers:\n",
      "  learn_throughput: 228.8\n",
      "  learn_time_ms: 17482.489\n",
      "  load_throughput: 23990299.333\n",
      "  load_time_ms: 0.167\n",
      "  restore_workers_time_ms: 0.0\n",
      "  sample_time_ms: 7428.703\n",
      "  synch_weights_time_ms: 10.798\n",
      "  training_iteration_time_ms: 24923.397\n",
      "  training_step_time_ms: 24923.397\n",
      "timestamp: 1720450980\n",
      "timesteps_total: 24000\n",
      "training_iteration: 6\n",
      "trial_id: default\n",
      "\n",
      "Training episode 6\n",
      "agent_timesteps_total: 28000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 28000\n",
      "  num_agent_steps_trained: 28000\n",
      "  num_env_steps_sampled: 28000\n",
      "  num_env_steps_trained: 28000\n",
      "custom_metrics: {}\n",
      "date: 2024-07-08_10-03-25\n",
      "done: false\n",
      "env_runners:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.005693197250366211\n",
      "    StateBufferConnector_ms: 0.003923177719116211\n",
      "    ViewRequirementAgentConnector_ms: 0.09806013107299805\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 61.0\n",
      "  episode_media: {}\n",
      "  episode_return_max: 6579.349349857072\n",
      "  episode_return_mean: 1982.6948258627879\n",
      "  episode_return_min: 442.83149476257057\n",
      "  episode_reward_max: 6579.349349857072\n",
      "  episode_reward_mean: 1982.6948258627879\n",
      "  episode_reward_min: 442.83149476257057\n",
      "  episodes_this_iter: 66\n",
      "  episodes_timesteps_total: 6100\n",
      "  hist_stats:\n",
      "    episode_lengths: [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61]\n",
      "    episode_reward: [5798.161235066025, 1715.5066144469379, 925.8046898533228, 1002.4685594392093,\n",
      "      2516.865576071446, 1510.1861754598528, 3761.6483049912986, 4286.359980782743,\n",
      "      4173.482593625416, 5620.183036621868, 4526.875605410164, 707.1319231811335,\n",
      "      818.8082207922256, 1152.570564155667, 966.2589295578596, 6579.349349857072,\n",
      "      657.7316752713065, 608.1334200352945, 1244.4457206190975, 1364.1923848205324,\n",
      "      591.4623236905386, 529.7475996045661, 857.0964874930874, 5527.557107232951,\n",
      "      3546.1080963893055, 1420.2209645326434, 2298.8925284898864, 1505.155479514905,\n",
      "      2589.9733239807015, 442.83149476257057, 3127.913407534624, 1423.6238319213512,\n",
      "      1519.5934595538013, 704.3798908524744, 1559.846286841482, 2205.293201594499,\n",
      "      4548.6512576041405, 4992.47651971508, 944.2412050018498, 5900.279729810258,\n",
      "      4034.331153843918, 658.4155007133805, 537.0722339145366, 1189.569254610341,\n",
      "      1497.1051772186938, 1495.4531903479296, 3580.458740187287, 3302.4725054613536,\n",
      "      2672.4381951234714, 2414.0141222623683, 1715.6465373052647, 1157.2187280723404,\n",
      "      849.1336683658458, 511.91776318957056, 546.7593875190571, 1194.5169898727836,\n",
      "      1227.3256110266484, 1600.2239587746556, 844.0044390881123, 460.72360480909697,\n",
      "      1514.7581347495936, 1038.9311610292539, 1811.0224828332537, 858.3853652798981,\n",
      "      1810.6100526639966, 1975.383992336653, 722.5681693432725, 803.125960264311,\n",
      "      2079.548132611513, 1166.2452764217287, 702.0444369236127, 715.4009051166432,\n",
      "      1650.2018363379277, 640.7543221305203, 639.2061530141391, 4636.927452115148,\n",
      "      2468.8675855210763, 543.1808126424035, 1895.9204510691757, 627.4955602592111,\n",
      "      4248.097254682817, 5567.464193318962, 546.6378232210938, 768.6743120939102,\n",
      "      1649.9163795399304, 2342.236419710815, 926.3401444514232, 874.6337421678282,\n",
      "      3409.1968115145028, 1311.6994929886414, 5930.2447415945135, 1144.1635305505129,\n",
      "      3391.1391074815015, 545.7215746944603, 824.9129642839763, 2198.336341313422,\n",
      "      538.6366190476615, 480.0852564176748, 1675.8812778716747, 3934.576838784304]\n",
      "  num_episodes: 66\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1859101368454673\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.3468179832218004\n",
      "    mean_inference_ms: 1.2851952925299006\n",
      "    mean_raw_obs_processing_ms: 0.7591371664454255\n",
      "episode_media: {}\n",
      "hostname: Brandons-PC\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.10000000000000002\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.215658079039666\n",
      "        entropy_coeff: 0.0\n",
      "        grad_gnorm: 1.400119492379568\n",
      "        kl: 0.008998103753929764\n",
      "        policy_loss: -0.02671360982211447\n",
      "        total_loss: 9.974186214323966\n",
      "        vf_explained_var: 0.00010161278068378407\n",
      "        vf_loss: 10.0\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "      num_grad_updates_lifetime: 6045.5\n",
      "  num_agent_steps_sampled: 28000\n",
      "  num_agent_steps_trained: 28000\n",
      "  num_env_steps_sampled: 28000\n",
      "  num_env_steps_trained: 28000\n",
      "iterations_since_restore: 7\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 28000\n",
      "num_agent_steps_sampled_lifetime: 28000\n",
      "num_agent_steps_trained: 28000\n",
      "num_env_steps_sampled: 28000\n",
      "num_env_steps_sampled_lifetime: 28000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_sampled_throughput_per_sec: 159.01459346007186\n",
      "num_env_steps_trained: 28000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_env_steps_trained_throughput_per_sec: 159.01459346007186\n",
      "num_healthy_workers: 2\n",
      "num_in_flight_async_sample_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 18.891666666666666\n",
      "  ram_util_percent: 90.3611111111111\n",
      "pid: 18244\n",
      "time_since_restore: 174.75703716278076\n",
      "time_this_iter_s: 25.163445711135864\n",
      "time_total_s: 174.75703716278076\n",
      "timers:\n",
      "  learn_throughput: 225.264\n",
      "  learn_time_ms: 17756.913\n",
      "  load_throughput: 27988682.555\n",
      "  load_time_ms: 0.143\n",
      "  restore_workers_time_ms: 0.0\n",
      "  sample_time_ms: 7187.056\n",
      "  synch_weights_time_ms: 11.297\n",
      "  training_iteration_time_ms: 24956.472\n",
      "  training_step_time_ms: 24956.472\n",
      "timestamp: 1720451005\n",
      "timesteps_total: 28000\n",
      "training_iteration: 7\n",
      "trial_id: default\n",
      "\n",
      "Training episode 7\n",
      "agent_timesteps_total: 32000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 32000\n",
      "  num_agent_steps_trained: 32000\n",
      "  num_env_steps_sampled: 32000\n",
      "  num_env_steps_trained: 32000\n",
      "custom_metrics: {}\n",
      "date: 2024-07-08_10-03-59\n",
      "done: false\n",
      "env_runners:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.006567478179931641\n",
      "    StateBufferConnector_ms: 0.0035130977630615234\n",
      "    ViewRequirementAgentConnector_ms: 0.10500121116638184\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 61.0\n",
      "  episode_media: {}\n",
      "  episode_return_max: 8253.671366893368\n",
      "  episode_return_mean: 1839.5243068740401\n",
      "  episode_return_min: 363.2953982186612\n",
      "  episode_reward_max: 8253.671366893368\n",
      "  episode_reward_mean: 1839.5243068740401\n",
      "  episode_reward_min: 363.2953982186612\n",
      "  episodes_this_iter: 66\n",
      "  episodes_timesteps_total: 6100\n",
      "  hist_stats:\n",
      "    episode_lengths: [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "      61, 61, 61, 61, 61, 61, 61, 61, 61]\n",
      "    episode_reward: [722.5681693432725, 803.125960264311, 2079.548132611513, 1166.2452764217287,\n",
      "      702.0444369236127, 715.4009051166432, 1650.2018363379277, 640.7543221305203,\n",
      "      639.2061530141391, 4636.927452115148, 2468.8675855210763, 543.1808126424035,\n",
      "      1895.9204510691757, 627.4955602592111, 4248.097254682817, 5567.464193318962,\n",
      "      546.6378232210938, 768.6743120939102, 1649.9163795399304, 2342.236419710815,\n",
      "      926.3401444514232, 874.6337421678282, 3409.1968115145028, 1311.6994929886414,\n",
      "      5930.2447415945135, 1144.1635305505129, 3391.1391074815015, 545.7215746944603,\n",
      "      824.9129642839763, 2198.336341313422, 538.6366190476615, 480.0852564176748,\n",
      "      1675.8812778716747, 3934.576838784304, 899.8211853441494, 1079.2864039947692,\n",
      "      4095.3132571171236, 2592.242922540335, 1590.9919625993853, 4105.9495028808005,\n",
      "      574.2310548261501, 1245.8029701684684, 1205.3065933986354, 2293.4928354513268,\n",
      "      834.893317054295, 795.9072550365336, 466.7567932346948, 716.6621375269361, 4412.9211857282335,\n",
      "      1386.1867414221902, 6697.250824995972, 3752.879600438177, 1315.9836626156343,\n",
      "      648.9147314602501, 1255.9026451714026, 1418.946953774467, 873.9726896876659,\n",
      "      2557.486115072497, 1763.300015319788, 2460.8640243973714, 554.4911959884914,\n",
      "      1096.6375130304032, 6132.368598224854, 457.34406115643964, 2241.808306666859,\n",
      "      885.8609091323858, 1169.752650702866, 1398.8334890051897, 3796.998538317266,\n",
      "      5150.728505567294, 668.0625055199943, 743.0271141990027, 1162.8302311031503,\n",
      "      4841.917336547514, 515.1515523659497, 1904.7011194737038, 1024.6846602648493,\n",
      "      543.50929264108, 771.7278605740509, 2341.384985188991, 560.492084256767, 8253.671366893368,\n",
      "      829.5966825833426, 658.4721375202336, 761.5064496999113, 1183.9015845191989,\n",
      "      1736.924214834762, 1249.7901833554438, 652.1312197268176, 747.3731464832996,\n",
      "      363.2953982186612, 1006.165265043373, 615.444851351196, 4253.123375596622, 4864.847777788099,\n",
      "      2679.5721612723487, 917.687608596586, 673.7115375193482, 999.900822403834, 901.6531333128737]\n",
      "  num_episodes: 66\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1804094318742152\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.3075418163823906\n",
      "    mean_inference_ms: 1.2496991415834233\n",
      "    mean_raw_obs_processing_ms: 0.73803002664464\n",
      "episode_media: {}\n",
      "hostname: Brandons-PC\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.10000000000000002\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.189087805055803\n",
      "        entropy_coeff: 0.0\n",
      "        grad_gnorm: 1.2568703669373706\n",
      "        kl: 0.010482951776809578\n",
      "        policy_loss: -0.021908674317021523\n",
      "        total_loss: 9.979139591545186\n",
      "        vf_explained_var: 8.626522556427986e-05\n",
      "        vf_loss: 10.0\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "      num_grad_updates_lifetime: 6975.5\n",
      "  num_agent_steps_sampled: 32000\n",
      "  num_agent_steps_trained: 32000\n",
      "  num_env_steps_sampled: 32000\n",
      "  num_env_steps_trained: 32000\n",
      "iterations_since_restore: 8\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 32000\n",
      "num_agent_steps_sampled_lifetime: 32000\n",
      "num_agent_steps_trained: 32000\n",
      "num_env_steps_sampled: 32000\n",
      "num_env_steps_sampled_lifetime: 32000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_sampled_throughput_per_sec: 117.87886152112235\n",
      "num_env_steps_trained: 32000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_env_steps_trained_throughput_per_sec: 117.87886152112235\n",
      "num_healthy_workers: 2\n",
      "num_in_flight_async_sample_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 24.84468085106383\n",
      "  ram_util_percent: 89.47659574468085\n",
      "pid: 18244\n",
      "time_since_restore: 208.69643330574036\n",
      "time_this_iter_s: 33.939396142959595\n",
      "time_total_s: 208.69643330574036\n",
      "timers:\n",
      "  learn_throughput: 209.683\n",
      "  learn_time_ms: 19076.442\n",
      "  load_throughput: 15991627.308\n",
      "  load_time_ms: 0.25\n",
      "  restore_workers_time_ms: 0.0\n",
      "  sample_time_ms: 6989.983\n",
      "  synch_weights_time_ms: 10.714\n",
      "  training_iteration_time_ms: 26078.556\n",
      "  training_step_time_ms: 26078.556\n",
      "timestamp: 1720451039\n",
      "timesteps_total: 32000\n",
      "training_iteration: 8\n",
      "trial_id: default\n",
      "\n",
      "Training episode 8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining episode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43malgo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(pretty_print(result))\n",
      "File \u001b[1;32mc:\\Users\\brand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ray\\tune\\trainable\\trainable.py:328\u001b[0m, in \u001b[0;36mTrainable.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    326\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 328\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    330\u001b[0m     skipped \u001b[38;5;241m=\u001b[39m skip_exceptions(e)\n",
      "File \u001b[1;32mc:\\Users\\brand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py:870\u001b[0m, in \u001b[0;36mAlgorithm.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    860\u001b[0m     (\n\u001b[0;32m    861\u001b[0m         train_results,\n\u001b[0;32m    862\u001b[0m         eval_results,\n\u001b[0;32m    863\u001b[0m         train_iter_ctx,\n\u001b[0;32m    864\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_one_training_iteration_and_evaluation_in_parallel()\n\u001b[0;32m    866\u001b[0m \u001b[38;5;66;03m# - No evaluation necessary, just run the next training iteration.\u001b[39;00m\n\u001b[0;32m    867\u001b[0m \u001b[38;5;66;03m# - We have to evaluate in this training iteration, but no parallelism ->\u001b[39;00m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;66;03m#   evaluate after the training iteration is entirely done.\u001b[39;00m\n\u001b[0;32m    869\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 870\u001b[0m     train_results, train_iter_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_one_training_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[38;5;66;03m# Sequential: Train (already done above), then evaluate.\u001b[39;00m\n\u001b[0;32m    873\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evaluate_this_iter \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mevaluation_parallel_to_training:\n",
      "File \u001b[1;32mc:\\Users\\brand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py:3160\u001b[0m, in \u001b[0;36mAlgorithm._run_one_training_iteration\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3156\u001b[0m \u001b[38;5;66;03m# Try to train one step.\u001b[39;00m\n\u001b[0;32m   3157\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timers[TRAINING_STEP_TIMER]:\n\u001b[0;32m   3158\u001b[0m     \u001b[38;5;66;03m# TODO (sven): Should we reduce the different\u001b[39;00m\n\u001b[0;32m   3159\u001b[0m     \u001b[38;5;66;03m#  `training_step_results` over time with MetricsLogger.\u001b[39;00m\n\u001b[1;32m-> 3160\u001b[0m     training_step_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training_step_results:\n\u001b[0;32m   3163\u001b[0m     results \u001b[38;5;241m=\u001b[39m training_step_results\n",
      "File \u001b[1;32mc:\\Users\\brand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ray\\rllib\\algorithms\\ppo\\ppo.py:428\u001b[0m, in \u001b[0;36mPPO.training_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_training_step_new_api_stack()\n\u001b[0;32m    425\u001b[0m \u001b[38;5;66;03m# Old and hybrid API stacks (Policy, RolloutWorker, Connector, maybe RLModule,\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;66;03m# maybe Learner).\u001b[39;00m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 428\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_training_step_old_and_hybrid_api_stacks\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\brand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ray\\rllib\\algorithms\\ppo\\ppo.py:596\u001b[0m, in \u001b[0;36mPPO._training_step_old_and_hybrid_api_stacks\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    594\u001b[0m     train_results \u001b[38;5;241m=\u001b[39m train_one_step(\u001b[38;5;28mself\u001b[39m, train_batch)\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 596\u001b[0m     train_results \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_gpu_train_one_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39menable_rl_module_and_learner:\n\u001b[0;32m    599\u001b[0m     \u001b[38;5;66;03m# The train results's loss keys are pids to their loss values. But we also\u001b[39;00m\n\u001b[0;32m    600\u001b[0m     \u001b[38;5;66;03m# return a total_loss key at the same level as the pid keys. So we need to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    603\u001b[0m     \u001b[38;5;66;03m#  passing medium to infer which policies to update. We could use\u001b[39;00m\n\u001b[0;32m    604\u001b[0m     \u001b[38;5;66;03m#  policies_to_train variable that is given by the user to infer this.\u001b[39;00m\n\u001b[0;32m    605\u001b[0m     policies_to_update \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(train_results\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m-\u001b[39m {ALL_MODULES}\n",
      "File \u001b[1;32mc:\\Users\\brand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ray\\rllib\\execution\\train_ops.py:176\u001b[0m, in \u001b[0;36mmulti_gpu_train_one_step\u001b[1;34m(algorithm, train_batch)\u001b[0m\n\u001b[0;32m    171\u001b[0m         permutation \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mpermutation(num_batches)\n\u001b[0;32m    172\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m batch_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[0;32m    173\u001b[0m             \u001b[38;5;66;03m# Learn on the pre-loaded data in the buffer.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m             \u001b[38;5;66;03m# Note: For minibatch SGD, the data is an offset into\u001b[39;00m\n\u001b[0;32m    175\u001b[0m             \u001b[38;5;66;03m# the pre-loaded entire train batch.\u001b[39;00m\n\u001b[1;32m--> 176\u001b[0m             results \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn_on_loaded_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpermutation\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mper_device_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[0;32m    178\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m             learner_info_builder\u001b[38;5;241m.\u001b[39madd_learn_on_batch_results(results, policy_id)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;66;03m# Tower reduce and finalize results.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\brand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ray\\rllib\\policy\\torch_policy_v2.py:838\u001b[0m, in \u001b[0;36mTorchPolicyV2.learn_on_loaded_batch\u001b[1;34m(self, offset, buffer_index)\u001b[0m\n\u001b[0;32m    835\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    836\u001b[0m         batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loaded_batches[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m][offset : offset \u001b[38;5;241m+\u001b[39m device_batch_size]\n\u001b[1;32m--> 838\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    840\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevices) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    841\u001b[0m     \u001b[38;5;66;03m# Copy weights of main model (tower-0) to all other towers.\u001b[39;00m\n\u001b[0;32m    842\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mstate_dict()\n",
      "File \u001b[1;32mc:\\Users\\brand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ray\\rllib\\utils\\threading.py:24\u001b[0m, in \u001b[0;36mwith_lock.<locals>.wrapper\u001b[1;34m(self, *a, **k)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m---> 24\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_lock\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\brand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ray\\rllib\\policy\\torch_policy_v2.py:715\u001b[0m, in \u001b[0;36mTorchPolicyV2.learn_on_batch\u001b[1;34m(self, postprocessed_batch)\u001b[0m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mon_learn_on_batch(\n\u001b[0;32m    710\u001b[0m     policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, train_batch\u001b[38;5;241m=\u001b[39mpostprocessed_batch, result\u001b[38;5;241m=\u001b[39mlearn_stats\n\u001b[0;32m    711\u001b[0m )\n\u001b[0;32m    713\u001b[0m \u001b[38;5;66;03m# Compute gradients (will calculate all losses and `backward()`\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;66;03m# them to get the grads).\u001b[39;00m\n\u001b[1;32m--> 715\u001b[0m grads, fetches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpostprocessed_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    717\u001b[0m \u001b[38;5;66;03m# Step the optimizers.\u001b[39;00m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_gradients(_directStepOptimizerSingleton)\n",
      "File \u001b[1;32mc:\\Users\\brand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ray\\rllib\\utils\\threading.py:24\u001b[0m, in \u001b[0;36mwith_lock.<locals>.wrapper\u001b[1;34m(self, *a, **k)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m---> 24\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_lock\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\brand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ray\\rllib\\policy\\torch_policy_v2.py:933\u001b[0m, in \u001b[0;36mTorchPolicyV2.compute_gradients\u001b[1;34m(self, postprocessed_batch)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_tensor_dict(postprocessed_batch, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevices[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    932\u001b[0m \u001b[38;5;66;03m# Do the (maybe parallelized) gradient calculation step.\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m tower_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_multi_gpu_parallel_grad_calc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpostprocessed_batch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    935\u001b[0m all_grads, grad_info \u001b[38;5;241m=\u001b[39m tower_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    937\u001b[0m grad_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallreduce_latency\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizers)\n",
      "File \u001b[1;32mc:\\Users\\brand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ray\\rllib\\policy\\torch_policy_v2.py:1429\u001b[0m, in \u001b[0;36mTorchPolicyV2._multi_gpu_parallel_grad_calc\u001b[1;34m(self, sample_batches)\u001b[0m\n\u001b[0;32m   1425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevices) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_fake_gpus\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m   1426\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m shard_idx, (model, sample_batch, device) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\n\u001b[0;32m   1427\u001b[0m         \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_gpu_towers, sample_batches, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevices)\n\u001b[0;32m   1428\u001b[0m     ):\n\u001b[1;32m-> 1429\u001b[0m         \u001b[43m_worker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1430\u001b[0m         \u001b[38;5;66;03m# Raise errors right away for better debugging.\u001b[39;00m\n\u001b[0;32m   1431\u001b[0m         last_result \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;28mlen\u001b[39m(results) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\brand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ray\\rllib\\policy\\torch_policy_v2.py:1347\u001b[0m, in \u001b[0;36mTorchPolicyV2._multi_gpu_parallel_grad_calc.<locals>._worker\u001b[1;34m(shard_idx, model, sample_batch, device)\u001b[0m\n\u001b[0;32m   1343\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1344\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m NullContextManager() \u001b[38;5;28;01mif\u001b[39;00m device\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m   1345\u001b[0m         device\n\u001b[0;32m   1346\u001b[0m     ):\n\u001b[1;32m-> 1347\u001b[0m         loss_out \u001b[38;5;241m=\u001b[39m \u001b[43mforce_list\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1348\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdist_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1349\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1351\u001b[0m         \u001b[38;5;66;03m# Call Model's custom-loss with Policy loss outputs and\u001b[39;00m\n\u001b[0;32m   1352\u001b[0m         \u001b[38;5;66;03m# train_batch.\u001b[39;00m\n\u001b[0;32m   1353\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\brand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ray\\rllib\\utils\\__init__.py:64\u001b[0m, in \u001b[0;36mforce_list\u001b[1;34m(elements, to_tuple)\u001b[0m\n\u001b[0;32m     59\u001b[0m         base \u001b[38;5;241m=\u001b[39m new_base\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m base\n\u001b[1;32m---> 64\u001b[0m \u001b[38;5;129m@DeveloperAPI\u001b[39m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforce_list\u001b[39m(elements\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, to_tuple\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     66\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m    Makes sure `elements` is returned as a list, whether `elements` is a single\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03m    item, already a list, or a tuple.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03m            returns an empty list/tuple.\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     80\u001b[0m     ctor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray.rllib.algorithms import ppo\n",
    "from ray.tune.logger import pretty_print\n",
    "import trading_env\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init()\n",
    "\n",
    "algo = ppo.PPO(env=trading_env.TradingEnv, config={\"env_config\": {\n",
    "    \"render_mode\" : None,\n",
    "    \"memory_length\" : 30,\n",
    "    \"episode_length\" : 90,\n",
    "    \"step_discount\" : \n",
    "}})\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"Training episode {i}\")\n",
    "    result = algo.train()\n",
    "    print(pretty_print(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved in directory C:\\Users\\brand\\AppData\\Local\\Temp\\tmpkap_4ue5\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = algo.save().checkpoint.path\n",
    "print(f\"Checkpoint saved in directory {checkpoint_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
